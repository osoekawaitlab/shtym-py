{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"shtym","text":"<p>AI-powered summary filter that distills any command's output.</p>"},{"location":"#overview","title":"Overview","text":"<p>Shtym is a command wrapper designed to reduce context size for both human users and AI coding agents. It wraps command execution and, when an LLM is available, summarizes the output; otherwise it passes output through unchanged.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install shtym\n\n# with Ollama support (requires a running Ollama instance)\npip install \"shtym[ollama]\"\n</code></pre>"},{"location":"#configuration","title":"Configuration","text":""},{"location":"#basic-configuration-environment-variables","title":"Basic Configuration (Environment Variables)","text":"<p>Configure Ollama settings using environment variables:</p> <ul> <li><code>SHTYM_LLM_SETTINGS__BASE_URL</code>: Ollama server URL (defaults to <code>http://localhost:11434</code>)</li> <li><code>SHTYM_LLM_SETTINGS__MODEL</code>: Model to use (defaults to <code>gpt-oss:20b</code>)</li> </ul> <p>Example:</p> <pre><code>export SHTYM_LLM_SETTINGS__BASE_URL=http://localhost:11434\nexport SHTYM_LLM_SETTINGS__MODEL=llama2\nstym run pytest tests/\n</code></pre>"},{"location":"#advanced-configuration-profiles","title":"Advanced Configuration (Profiles)","text":"<p>For more control, create profiles in <code>~/.config/shtym/profiles.toml</code> with custom prompts and LLM settings:</p> <pre><code>[profiles.summary]\ntype = \"llm\"\nsystem_prompt_template = \"You are summarizing: $command\"\nuser_prompt_template = \"Output:\\n$stdout\"\n\n[profiles.summary.llm_settings]\nmodel_name = \"gpt-oss:20b\"\nbase_url = \"http://localhost:11434\"\n</code></pre> <p>Use profiles with the <code>--profile</code> option:</p> <pre><code>stym run --profile summary pytest tests/\n</code></pre> <p>See the Profiles documentation for more details.</p>"},{"location":"#usage","title":"Usage","text":"<p>Wrap any command with <code>stym run</code>:</p> <pre><code># Run tests\nstym run pytest tests/\n\n# Run linter\nstym run ruff check .\n\n# Build project\nstym run npm run build\n\n# Any command with options\nstym run ls -la\n\n# Pipe output to other commands\nstym run pytest tests/ | grep FAILED\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Exit code inheritance: Shtym preserves the wrapped command's exit code, making it CI/CD friendly</li> <li>Clean stdout: Output contains only command results, no progress indicators or metadata</li> <li>Transparent wrapper: Works seamlessly with existing workflows and scripts</li> <li>Optional LLM summaries: If Ollama is available, output is summarized by the configured model; otherwise passthrough is used automatically</li> </ul>"},{"location":"#design-philosophy","title":"Design Philosophy","text":"<p>Shtym follows Unix conventions for command wrappers (like <code>sudo</code>, <code>timeout</code>, <code>time</code>):</p> <ul> <li>Executes commands as subprocesses</li> <li>Inherits and propagates exit codes exactly</li> <li>Maintains clean stdout for composability</li> <li>Enables reliable integration with automated workflows</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Profiles - Advanced configuration with custom profiles</li> <li>Roadmap - Planned features and enhancements</li> </ul>"},{"location":"#development","title":"Development","text":"<p>For development documentation, see:</p> <ul> <li>Development Guide - Setup, testing, and contribution guidelines</li> <li>Architecture Overview - Design principles and ADRs</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT</p>"},{"location":"configuration/","title":"Configuration","text":"<p>Shtym uses environment variables for configuration.</p>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"configuration/#shtym_llm_settings__base_url","title":"<code>SHTYM_LLM_SETTINGS__BASE_URL</code>","text":"<p>Description: Ollama server URL</p> <p>Default: <code>http://localhost:11434</code></p> <p>Example:</p> <pre><code>export SHTYM_LLM_SETTINGS__BASE_URL=http://ollama.example.com:11434\n</code></pre>"},{"location":"configuration/#shtym_llm_settings__model","title":"<code>SHTYM_LLM_SETTINGS__MODEL</code>","text":"<p>Description: Ollama model name to use for output summarization</p> <p>Default: <code>gpt-oss:20b</code></p> <p>Example:</p> <pre><code>export SHTYM_LLM_SETTINGS__MODEL=llama2\n</code></pre> <p>Notes:</p> <ul> <li>Empty strings or whitespace-only values are treated as unset (falls back to default)</li> <li>If the specified model is not available in Ollama, shtym silently falls back to PassThrough mode</li> </ul>"},{"location":"development/","title":"Development Guide","text":"<p>This guide covers the development workflow for shtym contributors.</p>"},{"location":"development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or later</li> <li>uv - Fast Python package installer and resolver</li> </ul>"},{"location":"development/#development-setup","title":"Development Setup","text":""},{"location":"development/#1-install-uv","title":"1. Install uv","text":"<pre><code># macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Or using pip\npip install uv\n</code></pre>"},{"location":"development/#2-clone-the-repository","title":"2. Clone the Repository","text":"<pre><code>git clone https://github.com/osoekawaitlab/shtym-py.git\ncd shtym-py\n</code></pre>"},{"location":"development/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code># Install development dependencies\nuv sync --group dev\n\n# Install with Ollama support\nuv pip install -e \".[ollama]\" --group dev\n\n# Install documentation dependencies\nuv sync --group docs\n</code></pre>"},{"location":"development/#project-structure","title":"Project Structure","text":"<pre><code>shtym-py/\n\u251c\u2500\u2500 src/shtym/              # Source code\n\u2502   \u251c\u2500\u2500 domain/             # Domain layer (business logic)\n\u2502   \u251c\u2500\u2500 infrastructure/     # Infrastructure layer (external integrations)\n\u2502   \u251c\u2500\u2500 application.py      # Application layer (orchestration)\n\u2502   \u251c\u2500\u2500 cli.py              # Presentation layer (CLI interface)\n\u2502   \u2514\u2500\u2500 exceptions.py       # Exception hierarchy\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 unit/               # Unit tests (mocked dependencies)\n\u2502   \u251c\u2500\u2500 e2e/                # End-to-end tests (with cassettes)\n\u2502   \u2514\u2500\u2500 fixtures/           # Test fixtures and cassettes\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 adr/                # Architecture Decision Records\n\u2502   \u2514\u2500\u2500 architecture/       # Architecture documentation\n\u251c\u2500\u2500 noxfile.py              # Task automation\n\u2514\u2500\u2500 pyproject.toml          # Project configuration\n</code></pre>"},{"location":"development/#architecture-layers","title":"Architecture Layers","text":"<p>Shtym follows a layered architecture (see ADR-0003):</p> <ul> <li>Presentation Layer (<code>cli.py</code>): Command-line interface</li> <li>Application Layer (<code>application.py</code>): Business logic orchestration</li> <li>Domain Layer (<code>domain/</code>): Core business concepts (Profile, Processor protocols)</li> <li>Infrastructure Layer (<code>infrastructure/</code>): External system integrations (file I/O, LLM clients)</li> </ul> <p>For detailed architecture documentation, see Architecture Overview.</p>"},{"location":"development/#running-tests","title":"Running Tests","text":"<p>Shtym uses nox for task automation. All test commands use uv as the backend.</p>"},{"location":"development/#unit-tests-only","title":"Unit Tests Only","text":"<pre><code># With Ollama support\nuv run nox -s tests_unit\n\n# Without Ollama dependency\nuv run nox -s tests_unit_no_ollama\n</code></pre>"},{"location":"development/#end-to-end-tests-only","title":"End-to-End Tests Only","text":"<pre><code>uv run nox -s tests_e2e\n</code></pre>"},{"location":"development/#all-tests-with-coverage","title":"All Tests with Coverage","text":"<pre><code># Runs all tests with coverage report (requires 80% minimum coverage)\nuv run nox -s tests\n\n# Coverage reports generated:\n# - Terminal: detailed coverage per file\n# - HTML: htmlcov/index.html\n</code></pre>"},{"location":"development/#test-across-all-python-versions","title":"Test Across All Python Versions","text":"<pre><code># Tests on Python 3.10, 3.11, 3.12, 3.13\nuv run nox -s tests_all_versions\n</code></pre>"},{"location":"development/#e2e-test-cassettes","title":"E2E Test Cassettes","text":"<p>E2E tests interact with external services (Ollama LLM server) using a record/replay mechanism called \"cassettes\". This allows tests to run without requiring a live Ollama instance.</p>"},{"location":"development/#what-are-cassettes","title":"What Are Cassettes?","text":"<p>Cassettes are JSON files that record HTTP requests and responses during test execution. They are stored in <code>tests/fixtures/cassettes/</code> and contain:</p> <ul> <li>HTTP request details (method, path, query, body, headers)</li> <li>HTTP response data (status, body, headers)</li> </ul> <p>Each cassette entry is keyed by a hash of the normalized request, ensuring consistent replay of identical requests.</p> <p>Example cassette location: <code>tests/fixtures/cassettes/test_profiles_toml/test_load_profile_from_toml_file.json</code></p>"},{"location":"development/#replay-mode-default","title":"Replay Mode (Default)","text":"<p>By default, E2E tests run in replay mode:</p> <pre><code># No Ollama server needed - uses recorded cassettes\nuv run pytest tests/e2e/\n</code></pre> <p>Behavior in replay mode:</p> <ul> <li>Tests send HTTP requests to a local mock server (pytest-httpserver)</li> <li>Mock server responds with data from cassette files</li> <li>No external dependencies required (Ollama doesn't need to be running)</li> <li>Tests run quickly and deterministically</li> <li>Cassette files must exist or tests will fail</li> </ul> <p>Use replay mode for:</p> <ul> <li>CI/CD pipelines</li> <li>Local development without Ollama</li> <li>Fast test execution</li> <li>Reproducible test results</li> </ul>"},{"location":"development/#record-mode","title":"Record Mode","text":"<p>When tests or Ollama interactions change, cassettes must be re-recorded:</p> <pre><code># Requires running Ollama instance with appropriate model\nSHTYMTEST_RECORDER_MODE=record uv run pytest tests/e2e/\n</code></pre> <p>Behavior in record mode:</p> <ul> <li>Tests send HTTP requests to local mock server (pytest-httpserver)</li> <li>Mock server forwards requests to real Ollama server</li> <li>Receives responses from Ollama and records request/response pairs to cassette files</li> <li>Overwrites existing cassettes with new recordings</li> <li>Requires Ollama server running at configured URL (default: <code>http://localhost:11434</code>)</li> </ul> <p>Prerequisites for recording:</p> <ol> <li>Ollama server must be running: <code>ollama serve</code></li> <li>Required model must be available: <code>ollama pull gpt-oss:20b</code> (or configured model)</li> <li>Environment variables set if using non-default configuration:</li> </ol> <pre><code>export SHTYM_LLM_SETTINGS__BASE_URL=http://localhost:11434\nexport SHTYM_LLM_SETTINGS__MODEL=gpt-oss:20b\n</code></pre> <p>When to record new cassettes:</p> <ul> <li>Adding new E2E tests that interact with Ollama</li> <li>Changing prompt templates or LLM interaction logic</li> <li>Updating to new Ollama API version</li> <li>Modifying test data that affects LLM requests</li> </ul> <p>After recording:</p> <ul> <li>Commit the updated cassette files to version control</li> <li>Verify tests still pass in replay mode: <code>uv run pytest tests/e2e/</code></li> <li>Review cassette diffs to ensure expected changes only</li> </ul>"},{"location":"development/#auto-mode","title":"Auto Mode","text":"<p>Auto mode intelligently switches between replay and record:</p> <pre><code># Replays from cassette when available, records when missing\nSHTYMTEST_RECORDER_MODE=auto uv run pytest tests/e2e/\n</code></pre> <p>Behavior in auto mode:</p> <ul> <li>If cassette entry exists for a request \u2192 replay from cassette (fast, no Ollama needed)</li> <li>If cassette entry missing for a request \u2192 forward to real Ollama server and record</li> <li>Automatically creates cassettes for new tests while using existing cassettes for unchanged tests</li> </ul> <p>Use auto mode for:</p> <ul> <li>Adding new tests incrementally (only records new interactions)</li> <li>Updating specific tests (only re-records changed interactions)</li> <li>Local development workflow (avoids repeatedly recording unchanged tests)</li> </ul> <p>Prerequisites:</p> <ul> <li>Same as record mode: Ollama server must be running with required model</li> </ul>"},{"location":"development/#code-quality","title":"Code Quality","text":""},{"location":"development/#linting","title":"Linting","text":"<pre><code># Check code style issues\nuv run nox -s lint\n\n# Or run directly\nuv run ruff check .\n</code></pre>"},{"location":"development/#formatting","title":"Formatting","text":"<pre><code># Auto-format code\nuv run nox -s format_code\n\n# Or run directly\nuv run ruff format .\n</code></pre>"},{"location":"development/#type-checking","title":"Type Checking","text":"<pre><code># Run mypy type checker\nuv run nox -s mypy\n\n# Or run directly\nuv run mypy src/ tests/\n</code></pre>"},{"location":"development/#configuration","title":"Configuration","text":"<ul> <li>Ruff: Configured in <code>pyproject.toml</code> with Google-style docstrings</li> <li>Mypy: Strict mode enabled with comprehensive type checking</li> <li>Pytest: Doctest modules, strict markers, random test order</li> </ul>"},{"location":"development/#building-documentation","title":"Building Documentation","text":"<pre><code># Build documentation site\nuv run nox -s docs_build\n\n# Serve locally (not in noxfile, run directly)\nuv run mkdocs serve\n</code></pre> <p>Documentation is built with MkDocs Material and deployed to GitHub Pages.</p>"},{"location":"development/#coding-standards","title":"Coding Standards","text":""},{"location":"development/#exception-handling","title":"Exception Handling","text":"<p>All infrastructure errors must extend <code>ShtymInfrastructureError</code> (see ADR-0017):</p> <pre><code>from shtym.exceptions import ShtymInfrastructureError\n\nclass FileReadError(ShtymInfrastructureError):\n    \"\"\"Exception raised when file reading fails.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        super().__init__(f\"File read error: {message}\")\n\n# Always use exception chaining\ntry:\n    with open(path) as f:\n        return f.read()\nexcept FileNotFoundError as e:\n    msg = f\"File not found: {path}\"\n    raise FileReadError(msg) from e\n</code></pre>"},{"location":"development/#silent-fallback-pattern","title":"Silent Fallback Pattern","text":"<p>When resources are unavailable (missing profiles, unavailable models), silently fall back to <code>PassThroughProcessor</code> (see ADR-0009 and ADR-0011):</p> <pre><code>try:\n    profile = repository.get(profile_name)\nexcept ProfileNotFoundError:\n    # Silent fallback - no warnings, no errors\n    return PassThroughProcessor()\n</code></pre>"},{"location":"development/#dependency-injection","title":"Dependency Injection","text":"<p>Use constructor injection for testability:</p> <pre><code>class FileBasedProfileRepository:\n    def __init__(self, file_reader: FileReader, parser: TOMLProfileParser) -&gt; None:\n        self.file_reader = file_reader\n        self.parser = parser\n</code></pre>"},{"location":"development/#test-organization","title":"Test Organization","text":"<ul> <li>Unit tests: Mock all external dependencies (file I/O, HTTP, LLM clients)</li> <li>E2E tests: Use recorded cassettes for external service interactions</li> <li>One test per behavior: Each test validates a single specific behavior</li> <li>Descriptive test names: <code>test_&lt;what&gt;_&lt;when&gt;_&lt;expected&gt;</code> (e.g., <code>test_get_profile_raises_error_when_not_found</code>)</li> </ul>"},{"location":"development/#contributing","title":"Contributing","text":""},{"location":"development/#before-submitting-a-pull-request","title":"Before Submitting a Pull Request","text":"<ol> <li>Run all tests: <code>uv run nox -s tests</code></li> <li>Check code quality: <code>uv run nox -s lint mypy</code></li> <li>Format code: <code>uv run nox -s format_code</code></li> <li>Update documentation: Add ADRs for architectural decisions</li> <li>Write tests: Maintain 80%+ coverage with meaningful tests</li> </ol>"},{"location":"development/#commit-messages","title":"Commit Messages","text":"<p>Follow conventional commit format:</p> <ul> <li><code>feat: add profile loading from TOML files</code></li> <li><code>fix: handle file read errors gracefully</code></li> <li><code>test: add E2E tests for profile loading</code></li> <li><code>docs: update development guide</code></li> <li><code>refactor: extract file reading logic</code></li> </ul>"},{"location":"development/#architecture-decision-records","title":"Architecture Decision Records","text":"<p>Document significant architectural decisions in <code>docs/adr/</code>:</p> <ol> <li>Use template: <code>docs/adr/0000-adr-template.md</code></li> <li>Number sequentially: <code>0018-title.md</code></li> <li>Update <code>docs/architecture/overview.md</code> with summary</li> <li>Focus on decisions, not implementations</li> <li>Document why alternatives were rejected</li> </ol>"},{"location":"development/#debugging","title":"Debugging","text":""},{"location":"development/#enable-verbose-logging","title":"Enable Verbose Logging","text":"<pre><code># Set environment variable for debug output\nexport SHTYM_DEBUG=1\nstym run pytest tests/\n</code></pre>"},{"location":"development/#inspect-test-cassettes","title":"Inspect Test Cassettes","text":"<p>E2E test cassettes are stored in <code>tests/fixtures/cassettes/</code>:</p> <pre><code># View cassette content\ncat tests/fixtures/cassettes/test_profiles_toml/test_load_profile_from_toml_file.json\n</code></pre>"},{"location":"development/#test-individual-files","title":"Test Individual Files","text":"<pre><code># Run specific test file\nuv run pytest tests/unit/test_application.py -v\n\n# Run specific test function\nuv run pytest tests/unit/test_application.py::test_create_application_with_default_profile -v\n</code></pre>"},{"location":"development/#release-process","title":"Release Process","text":"<p>(To be documented when release workflow is established)</p>"},{"location":"development/#getting-help","title":"Getting Help","text":"<ul> <li>Issues: GitHub Issues</li> <li>Architecture: See Architecture Overview and ADRs in <code>docs/adr/</code></li> <li>Project Goals: See Home</li> </ul>"},{"location":"profiles/","title":"Profiles","text":"<p>Profiles allow you to define custom output transformation behaviors for different use cases. Instead of relying solely on environment variables, you can create named profiles with specific LLM settings and prompt templates.</p>"},{"location":"profiles/#what-are-profiles","title":"What Are Profiles?","text":"<p>A profile is a named configuration that defines:</p> <ul> <li>System prompt template: Context and instructions for the LLM (recommended: keep concise)</li> <li>User prompt template: Template for command output and user message</li> <li>LLM settings: Model name and server URL to use</li> <li>Processing behavior: How command output should be transformed</li> </ul> <p>Profiles enable you to:</p> <ul> <li>Use different LLM models for different tasks (e.g., fast model for summaries, powerful model for analysis)</li> <li>Apply task-specific prompts (e.g., \"summarize\", \"translate\", \"extract errors\")</li> <li>Switch between configurations without changing environment variables</li> </ul>"},{"location":"profiles/#profile-configuration-file","title":"Profile Configuration File","text":"<p>Profiles are defined in a TOML file located at:</p> <pre><code>~/.config/shtym/profiles.toml\n</code></pre> <p>If this file doesn't exist, shtym uses the built-in default profile that reads from environment variables (see Configuration).</p>"},{"location":"profiles/#basic-profile-syntax","title":"Basic Profile Syntax","text":"<p>A profile definition has the following structure:</p> <pre><code>[profiles.&lt;profile_name&gt;]\ntype = \"llm\"\nversion = 1\nsystem_prompt_template = \"&lt;system instructions&gt;\"\nuser_prompt_template = \"&lt;user message template&gt;\"\n\n[profiles.&lt;profile_name&gt;.llm_settings]\nmodel_name = \"&lt;model name&gt;\"\nbase_url = \"&lt;ollama server URL&gt;\"\n</code></pre>"},{"location":"profiles/#fields","title":"Fields","text":"<ul> <li><code>type</code>: Must be <code>\"llm\"</code> (currently the only supported type)</li> <li><code>version</code>: Profile schema version (currently <code>1</code>, optional with default)</li> <li><code>system_prompt_template</code>: Template for system prompt (sets LLM context)<ul> <li>Available variables: <code>$command</code>, <code>$stdout</code>, <code>$stderr</code></li> <li>Best practice: Keep concise; avoid <code>$stdout</code> for long outputs</li> <li>Example: <code>\"You are summarizing output from: $command\"</code></li> </ul> </li> <li><code>user_prompt_template</code>: Template for user message (contains command output)<ul> <li>Available variables: <code>$command</code>, <code>$stdout</code>, <code>$stderr</code></li> <li>Example: <code>\"Output:\\n$stdout\\n\\nErrors:\\n$stderr\"</code></li> </ul> </li> <li><code>model_name</code>: Ollama model to use (e.g., <code>\"gpt-oss:20b\"</code>, <code>\"llama2\"</code>)</li> <li><code>base_url</code>: Ollama server URL (e.g., <code>\"http://localhost:11434\"</code>)</li> </ul>"},{"location":"profiles/#creating-profiles","title":"Creating Profiles","text":""},{"location":"profiles/#example-summary-profile","title":"Example: Summary Profile","text":"<p>Create a profile for summarizing command output:</p> <pre><code>[profiles.summary]\ntype = \"llm\"\nsystem_prompt_template = \"Summarize output from: $command\"\nuser_prompt_template = \"Provide a 2-3 sentence summary.\\n\\n$stdout\"\n\n[profiles.summary.llm_settings]\nmodel_name = \"gpt-oss:20b\"\nbase_url = \"http://localhost:11434\"\n</code></pre> <p>Usage:</p> <pre><code>stym run --profile summary pytest tests/\n</code></pre>"},{"location":"profiles/#example-error-extraction-profile","title":"Example: Error Extraction Profile","text":"<p>Create a profile that extracts only errors:</p> <pre><code>[profiles.errors]\ntype = \"llm\"\nsystem_prompt_template = \"Extract errors from: $command\"\nuser_prompt_template = \"List only the errors.\\n\\nOutput:\\n$stdout\\n\\nErrors:\\n$stderr\"\n\n[profiles.errors.llm_settings]\nmodel_name = \"gpt-oss:120b\"  # Use more powerful model for accuracy\nbase_url = \"http://localhost:11434\"\n</code></pre> <p>Usage:</p> <pre><code>stym run --profile errors npm run build\n</code></pre>"},{"location":"profiles/#example-translation-profile","title":"Example: Translation Profile","text":"<p>Create a profile for translating output to another language:</p> <pre><code>[profiles.translate-ja]\ntype = \"llm\"\nsystem_prompt_template = \"Translate to Japanese: $command\"\nuser_prompt_template = \"$stdout\"\n\n[profiles.translate-ja.llm_settings]\nmodel_name = \"llama2\"\nbase_url = \"http://localhost:11434\"\n</code></pre> <p>Usage:</p> <pre><code>stym run --profile translate-ja echo \"Hello, world!\"\n</code></pre>"},{"location":"profiles/#multiple-profiles","title":"Multiple Profiles","text":"<p>You can define multiple profiles in the same file:</p> <pre><code>[profiles.summary]\ntype = \"llm\"\nsystem_prompt_template = \"Summarize: $command\"\nuser_prompt_template = \"$stdout\"\n\n[profiles.summary.llm_settings]\nmodel_name = \"gpt-oss:20b\"\nbase_url = \"http://localhost:11434\"\n\n[profiles.detailed]\ntype = \"llm\"\nsystem_prompt_template = \"Provide detailed analysis: $command\"\nuser_prompt_template = \"$stdout\"\n\n[profiles.detailed.llm_settings]\nmodel_name = \"gpt-oss:120b\"\nbase_url = \"http://localhost:11434\"\n\n[profiles.fast]\ntype = \"llm\"\nsystem_prompt_template = \"Quick summary: $command\"\nuser_prompt_template = \"$stdout\"\n\n[profiles.fast.llm_settings]\nmodel_name = \"gemma3:4b\"\nbase_url = \"http://localhost:11434\"\n</code></pre> <p>Switch between profiles using <code>--profile</code>:</p> <pre><code># Use fast profile for quick feedback\nstym run --profile fast pytest tests/\n\n# Use detailed profile for thorough analysis\nstym run --profile detailed ruff check .\n</code></pre>"},{"location":"profiles/#overriding-the-default-profile","title":"Overriding the Default Profile","text":"<p>The default profile is used when <code>--profile</code> is not specified:</p> <pre><code># Uses default profile\nstym run pytest tests/\n</code></pre> <p>By default, this uses the built-in profile that reads from environment variables. You can override it by defining a <code>default</code> profile in <code>profiles.toml</code>:</p> <pre><code>[profiles.default]\ntype = \"llm\"\nsystem_prompt_template = \"Summarize concisely: $command\"\nuser_prompt_template = \"$stdout\"\n\n[profiles.default.llm_settings]\nmodel_name = \"gpt-oss:20b\"\nbase_url = \"http://localhost:11434\"\n</code></pre> <p>After defining this, <code>stym run</code> without <code>--profile</code> will use your custom default settings instead of environment variables.</p>"},{"location":"profiles/#using-profiles","title":"Using Profiles","text":""},{"location":"profiles/#specify-profile-explicitly","title":"Specify Profile Explicitly","text":"<pre><code># Use named profile\nstym run --profile summary pytest tests/\n\n# Use default profile (explicit)\nstym run --profile default ruff check .\n</code></pre>"},{"location":"profiles/#use-default-profile-implicitly","title":"Use Default Profile Implicitly","text":"<pre><code># Uses default profile (from profiles.toml or built-in)\nstym run pytest tests/\n</code></pre>"},{"location":"profiles/#profile-selection-priority","title":"Profile Selection Priority","text":"<ol> <li><code>--profile</code> option: If specified, uses the named profile from <code>profiles.toml</code></li> <li>Default profile in <code>profiles.toml</code>: If <code>[profiles.default]</code> exists, uses it when <code>--profile</code> is not specified</li> <li>Built-in default: Uses environment variables (<code>SHTYM_LLM_SETTINGS__*</code>) as fallback</li> </ol>"},{"location":"profiles/#graceful-degradation","title":"Graceful Degradation","text":"<p>Following shtym's philosophy of graceful degradation:</p> <ul> <li>Missing profile file: Falls back to built-in default (environment variables)</li> <li>Invalid TOML syntax: Behaves as if no profiles are defined (silent fallback)</li> <li>Profile not found: Falls back to PassThrough mode (no error)</li> <li>Model unavailable: Falls back to PassThrough mode (outputs original command result)</li> </ul> <p>This ensures shtym never fails due to configuration issues.</p>"},{"location":"profiles/#best-practices","title":"Best Practices","text":""},{"location":"profiles/#1-use-descriptive-profile-names","title":"1. Use Descriptive Profile Names","text":"<pre><code># Good: Clear purpose\n[profiles.test-summary]\n[profiles.error-extractor]\n[profiles.translate-ja]\n\n# Avoid: Unclear purpose\n[profiles.profile1]\n[profiles.temp]\n</code></pre>"},{"location":"profiles/#2-match-model-power-to-task-complexity","title":"2. Match Model Power to Task Complexity","text":"<pre><code># Simple summarization: Use faster model\n[profiles.quick-summary]\ntype = \"llm\"\nsystem_prompt_template = \"One sentence summary: $command\"\nuser_prompt_template = \"$stdout\"\n[profiles.quick-summary.llm_settings]\nmodel_name = \"gemma3:4b\"\n\n# Complex analysis: Use powerful model\n[profiles.deep-analysis]\ntype = \"llm\"\nsystem_prompt_template = \"Comprehensive analysis: $command\"\nuser_prompt_template = \"Provide recommendations.\\n\\n$stdout\"\n[profiles.deep-analysis.llm_settings]\nmodel_name = \"gpt-oss:120b\"\n</code></pre>"},{"location":"profiles/#3-keep-prompts-focused","title":"3. Keep Prompts Focused","text":"<pre><code># Good: Specific instruction\nsystem_prompt_template = \"Extract error messages and line numbers: $command\"\nuser_prompt_template = \"$stdout\"\n\n# Avoid: Too vague\nsystem_prompt_template = \"Do something with this: $command\"\nuser_prompt_template = \"$stdout\"\n</code></pre>"},{"location":"profiles/#4-test-profiles-before-committing","title":"4. Test Profiles Before Committing","text":"<pre><code># Create test profile\necho '[profiles.test]\ntype = \"llm\"\nsystem_prompt_template = \"Test: $command\"\nuser_prompt_template = \"$stdout\"\n[profiles.test.llm_settings]\nmodel_name = \"gpt-oss:20b\"\nbase_url = \"http://localhost:11434\"' &gt;&gt; ~/.config/shtym/profiles.toml\n\n# Test it\nstym run --profile test echo \"Hello, world!\"\n</code></pre>"},{"location":"profiles/#troubleshooting","title":"Troubleshooting","text":""},{"location":"profiles/#profile-not-working","title":"Profile Not Working","text":"<p>Symptom: Profile seems ignored; output is not transformed</p> <p>Possible causes:</p> <ol> <li>TOML syntax error \u2192 Check file with TOML validator</li> <li>Profile name mismatch \u2192 Verify <code>--profile</code> matches <code>[profiles.&lt;name&gt;]</code></li> <li>Ollama server not running \u2192 Start Ollama: <code>ollama serve</code></li> <li>Model not available \u2192 Pull model: <code>ollama pull &lt;model_name&gt;</code></li> </ol> <p>Debug steps:</p> <pre><code># Verify TOML syntax\npython3 - &lt;&lt;'PY'\nimport sys\nfrom pathlib import Path\nif sys.version_info &gt;= (3, 11):\n    import tomllib\nelse:\n    import tomli as tomllib\nwith open(Path.home() / \".config\" / \"shtym\" / \"profiles.toml\", \"rb\") as f:\n    tomllib.load(f)\nprint(\"OK\")\nPY\n\n# Check Ollama server\ncurl http://localhost:11434/api/tags\n\n# Test with built-in default\nstym run echo \"test\"\n</code></pre>"},{"location":"profiles/#profile-file-location","title":"Profile File Location","text":"<p>Linux/macOS: <code>~/.config/shtym/profiles.toml</code></p> <p>Example:</p> <pre><code># Create directory if it doesn't exist\nmkdir -p ~/.config/shtym\n\n# Edit profiles\nnano ~/.config/shtym/profiles.toml\n</code></pre>"},{"location":"profiles/#viewing-active-configuration","title":"Viewing Active Configuration","text":"<p>Currently shtym does not have a command to show active profiles. This may be added in a future version.</p>"},{"location":"profiles/#examples","title":"Examples","text":""},{"location":"profiles/#cicd-profile","title":"CI/CD Profile","text":"<p>For continuous integration environments where you want concise summaries:</p> <pre><code>[profiles.ci]\ntype = \"llm\"\nsystem_prompt_template = \"Summarize test results, highlight failures: $command\"\nuser_prompt_template = \"$stdout\"\n\n[profiles.ci.llm_settings]\nmodel_name = \"gpt-oss:20b\"\nbase_url = \"http://ci-ollama.internal:11434\"\n</code></pre> <p>Usage in CI:</p> <pre><code>stym run --profile ci pytest --verbose tests/\n</code></pre>"},{"location":"profiles/#development-profile","title":"Development Profile","text":"<p>For local development with detailed feedback:</p> <pre><code>[profiles.dev]\ntype = \"llm\"\nsystem_prompt_template = \"Analyze and suggest improvements: $command\"\nuser_prompt_template = \"$stdout\"\n\n[profiles.dev.llm_settings]\nmodel_name = \"gpt-oss:120b\"\nbase_url = \"http://localhost:11434\"\n</code></pre>"},{"location":"profiles/#multilingual-support","title":"Multilingual Support","text":"<p>Define profiles for different languages:</p> <pre><code>[profiles.ja]\ntype = \"llm\"\nsystem_prompt_template = \"\u4ee5\u4e0b\u306e\u51fa\u529b\u3092\u65e5\u672c\u8a9e\u3067\u8981\u7d04\u3057\u3066\u304f\u3060\u3055\u3044: $command\"\nuser_prompt_template = \"$stdout\"\n\n[profiles.ja.llm_settings]\nmodel_name = \"gpt-oss:20b\"\nbase_url = \"http://localhost:11434\"\n\n[profiles.en]\ntype = \"llm\"\nsystem_prompt_template = \"Summarize the following output in English: $command\"\nuser_prompt_template = \"$stdout\"\n\n[profiles.en.llm_settings]\nmodel_name = \"gpt-oss:20b\"\nbase_url = \"http://localhost:11434\"\n</code></pre>"},{"location":"profiles/#future-enhancements","title":"Future Enhancements","text":"<p>The profile system will continue to evolve. Planned features include:</p> <ul> <li>Environment variable expansion: Use <code>${VAR_NAME}</code> syntax in profiles for secrets and per-environment values</li> <li>Project-local profiles: Place profiles in <code>.shtym/profiles.toml</code> for team-shared configurations</li> <li>Additional LLM providers: OpenAI, Anthropic (Claude), Azure OpenAI Service support</li> <li>Azure Key Vault integration: Fetch secrets using <code>akv://vault-name/secret-name</code> URI scheme via envresolve</li> </ul> <p>See the Roadmap for details and priorities.</p>"},{"location":"profiles/#see-also","title":"See Also","text":"<ul> <li>Configuration - Environment variable configuration</li> <li>Roadmap - Planned features and enhancements</li> <li>Development Guide - Contributing to shtym</li> <li>Architecture Overview - Design decisions behind profiles</li> </ul>"},{"location":"roadmap/","title":"Roadmap","text":"<p>This document outlines planned features and enhancements for shtym. Timing and scope may change based on feedback and contributions.</p>"},{"location":"roadmap/#near-term-high-priority","title":"Near Term (High Priority)","text":""},{"location":"roadmap/#project-local-profile-configuration","title":"Project-Local Profile Configuration","text":"<p>Support profile files in project directories (e.g., <code>.shtym/profiles.toml</code>), not just <code>~/.config/shtym/</code>.</p> <p>Why: Enable team-shared configurations committed to version control, with automatic profile discovery per project.</p> <p>Example:</p> <pre><code>my-project/\n\u251c\u2500\u2500 .shtym/profiles.toml  # Team profiles\n\u2514\u2500\u2500 src/\n\ncd my-project\nstym run --profile team-standard pytest  # Automatically uses project profiles\n</code></pre>"},{"location":"roadmap/#profile-inheritance","title":"Profile Inheritance","text":"<p>Allow profiles to extend other profiles to reduce duplication.</p> <p>Example:</p> <pre><code>[profiles.base-summary]\nsystem_prompt_template = \"Summarize: $command\"\nuser_prompt_template = \"$stdout\"\n\n[profiles.fast-summary]\nextends = \"base-summary\"\n[profiles.fast-summary.llm_settings]\nmodel_name = \"gemma3:4b\"\n</code></pre> <p>Why: Maintain related profiles more easily with shared configuration.</p>"},{"location":"roadmap/#environment-variable-expansion-in-profiles","title":"Environment Variable Expansion in Profiles","text":"<p>Enable <code>${VAR_NAME}</code> syntax in profile configurations to separate secrets from configuration files.</p> <p>Why: Keep API keys and credentials out of version control while sharing profile structure with teams.</p> <p>Example:</p> <pre><code>[profiles.openai]\ntype = \"llm\"\n[profiles.openai.llm_settings]\napi_key = \"${OPENAI_API_KEY}\"\nbase_url = \"${OPENAI_BASE_URL:-https://api.openai.com/v1}\"\n</code></pre>"},{"location":"roadmap/#structured-output-support","title":"Structured Output Support","text":"<p>Enable LLM responses in structured formats (JSON) with schema validation.</p> <p>Why: Make LLM outputs machine-readable for downstream processing, CI/CD integration, and programmatic analysis.</p> <p>Example:</p> <pre><code>[profiles.test-analysis]\ntype = \"llm\"\nsystem_prompt_template = \"Analyze test results: $command\"\nuser_prompt_template = \"$stdout\"\noutput_format = \"json\"\noutput_schema = \"\"\"\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"total_tests\": {\"type\": \"integer\"},\n    \"passed\": {\"type\": \"integer\"},\n    \"failed\": {\"type\": \"integer\"},\n    \"errors\": {\n      \"type\": \"array\",\n      \"items\": {\"type\": \"string\"}\n    }\n  }\n}\n\"\"\"\n</code></pre> <p>Use cases:</p> <ul> <li>Parse test results for CI/CD metrics</li> <li>Extract structured lint findings</li> <li>Generate machine-readable summaries for automation</li> </ul>"},{"location":"roadmap/#template-variable-support","title":"Template Variable Support","text":"<p>Enable user-defined variables in prompt templates with default values, allowing dynamic customization at runtime.</p> <p>Why: Reuse the same profile with different parameters without creating multiple profiles. Useful for automation tools like nox.</p> <p>Example:</p> <pre><code>[profiles.custom-summary]\ntype = \"llm\"\nsystem_prompt_template = \"Summarize from ${perspective:-general} perspective: $command\"\nuser_prompt_template = \"$stdout\"\n</code></pre> <p>Usage:</p> <pre><code># Use default value (general)\nstym run --profile custom-summary pytest tests/\n\n# Override with specific perspective\nstym run --profile custom-summary --var perspective=security pytest tests/\nstym run --profile custom-summary --var perspective=performance ruff check .\n</code></pre>"},{"location":"roadmap/#long-context-handling","title":"Long Context Handling","text":"<p>Support processing large command outputs that exceed LLM context limits through chunking and hierarchical summarization.</p> <p>Why: Enable shtym to handle real-world scenarios with massive test suites, large diffs, or extensive logs without hitting token limits.</p> <p>Example strategies:</p> <ul> <li>Chunk large outputs and process separately</li> <li>Hierarchical summarization (summarize chunks, then summarize summaries)</li> <li>Extract critical sections (errors, failures) before processing</li> <li>Stream processing for very large outputs</li> </ul> <p>Use cases:</p> <ul> <li>Large test suites with thousands of tests</li> <li>Extensive log files</li> <li>Large git diffs across many files</li> <li>Build outputs from complex projects</li> </ul>"},{"location":"roadmap/#medium-term","title":"Medium Term","text":""},{"location":"roadmap/#openai-api-support","title":"OpenAI API Support","text":"<p>Add support for OpenAI's API as an LLM provider alongside Ollama.</p> <p>Why: Provide flexibility to use different LLM providers based on availability, cost, or performance needs.</p>"},{"location":"roadmap/#azure-key-vault-integration","title":"Azure Key Vault Integration","text":"<p>Fetch secrets from Azure Key Vault using <code>akv://</code> URI scheme (via envresolve library).</p> <p>Why: Enterprise-grade secret management with audit trails and automatic rotation support.</p> <p>Example:</p> <pre><code>api_key = \"akv://my-vault/openai-key\"\n# Or with variable expansion:\napi_key = \"akv://${VAULT_NAME}/openai-key\"\n</code></pre>"},{"location":"roadmap/#long-term-lower-priority","title":"Long Term (Lower Priority)","text":""},{"location":"roadmap/#command-specific-prompt-presets","title":"Command-Specific Prompt Presets","text":"<p>Provide built-in optimized prompts for common development commands (pytest, ruff, git, npm, etc.) with automatic detection.</p> <p>Why: Make it easier to get started with effective prompts without manual prompt engineering.</p> <p>Example:</p> <pre><code>stym run pytest tests/  # Automatically uses pytest-optimized prompt\n</code></pre>"},{"location":"roadmap/#external-prompt-import","title":"External Prompt Import","text":"<p>Enable importing prompt configurations from URLs, files, or repositories.</p> <p>Why: Share and reuse proven prompt configurations across teams and projects.</p> <p>Example:</p> <pre><code>stym profile import https://example.com/prompts/pytest-detailed.toml\n</code></pre>"},{"location":"roadmap/#additional-llm-providers","title":"Additional LLM Providers","text":"<p>Extend support to Anthropic (Claude), Azure OpenAI Service, and other major LLM providers.</p> <p>Why: Maximize flexibility and support diverse deployment scenarios.</p>"},{"location":"roadmap/#configuration-management-commands","title":"Configuration Management Commands","text":"<p>Add <code>stym config</code> subcommands for validation and inspection:</p> <ul> <li><code>stym config validate</code> - Check profile syntax and configuration</li> <li><code>stym config list</code> - Show all available profiles</li> <li><code>stym config show &lt;profile&gt;</code> - Display profile details</li> </ul> <p>Why: Improve debugging and discoverability of profiles.</p>"},{"location":"roadmap/#contributing","title":"Contributing","text":"<p>Interested in implementing a feature? Please open a GitHub Issue to discuss the approach before starting work.</p>"},{"location":"roadmap/#feedback","title":"Feedback","text":"<p>Have ideas for other features? Open an issue or start a discussion.</p>"},{"location":"adr/0000-adr-template/","title":"ADR 0000: This is ADR Template and Short Title of the Architectural Decision Goes Here","text":""},{"location":"adr/0000-adr-template/#status","title":"Status","text":"<p>[Proposed | Accepted | Deprecated | Superseded]</p>"},{"location":"adr/0000-adr-template/#date","title":"Date","text":"<p>YYYY-MM-DD</p>"},{"location":"adr/0000-adr-template/#context","title":"Context","text":"<p>Describe the context and problem statement. What is the architectural challenge that needs to be addressed? Include any relevant constraints or requirements that influenced the decision.</p>"},{"location":"adr/0000-adr-template/#decision","title":"Decision","text":"<p>State the architectural decision clearly and concisely. What specific approach, technology, pattern, or solution was chosen?</p>"},{"location":"adr/0000-adr-template/#rationale","title":"Rationale","text":"<p>Explain the reasoning that led to this decision. Why was this particular option selected among the alternatives? Include relevant factors such as:</p> <ul> <li>Technical considerations</li> <li>Business requirements</li> <li>Team capabilities</li> <li>Time constraints</li> <li>Cost implications</li> </ul>"},{"location":"adr/0000-adr-template/#implications","title":"Implications","text":""},{"location":"adr/0000-adr-template/#positive-implications","title":"Positive Implications","text":"<p>List the benefits and positive outcomes expected from this decision.</p>"},{"location":"adr/0000-adr-template/#concerns","title":"Concerns","text":"<p>List potential challenges, risks, or negative consequences along with possible mitigation strategies.</p>"},{"location":"adr/0000-adr-template/#alternatives","title":"Alternatives","text":"<p>Describe other options that were considered and why they were not selected. For each alternative, briefly explain:</p> <ul> <li>Key characteristics</li> <li>Pros and cons relative to the chosen solution</li> <li>Reasons for rejection</li> </ul>"},{"location":"adr/0000-adr-template/#future-direction","title":"Future Direction","text":"<p>Outline any follow-up actions, future considerations, or potential changes that might be necessary as a result of this decision. Include potential triggers for revisiting this decision.</p>"},{"location":"adr/0000-adr-template/#references","title":"References","text":"<p>List any relevant documents, articles, books, or other resources that supported this decision:</p> <ul> <li>Links to relevant documentation</li> <li>Research materials</li> <li>Benchmarks or performance data</li> <li>Team discussions or meeting notes</li> </ul>"},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/","title":"ADR 0001: Keep stdout Clean for AI and Human Consumption","text":""},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/#status","title":"Status","text":"<p>Accepted (Amended 2025-11-21)</p>"},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/#date","title":"Date","text":"<p>2025-11-20</p>"},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/#context","title":"Context","text":"<p>The shtym tool is designed as a command-line filter that summarizes input using AI. Its primary purpose is to reduce context size for both human users and AI coding agents working in terminal environments. Users need to pipe command output through shtym and receive only the essential summary without any visual clutter or formatting.</p> <p>In modern development workflows, AI coding agents are increasingly consuming command-line output programmatically. Additionally, humans often need to copy-paste terminal output or chain commands together. Any extra formatting, progress indicators, or decorative elements in stdout would interfere with these use cases and defeat the purpose of context reduction.</p>"},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/#decision","title":"Decision","text":"<p>Keep stdout exclusively for AI-generated summaries. All other output (progress indicators, error messages, logging, metadata) must go to stderr or other channels.</p>"},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/#rationale","title":"Rationale","text":"<ul> <li>Context reduction goal: Adding any extra formatting to stdout would contradict the core purpose of reducing context size</li> <li>AI agent compatibility: AI coding agents can cleanly consume stdout without parsing decorative elements</li> <li>Unix philosophy: Following the principle that stdout carries the primary data output while stderr carries metadata</li> <li>Composability: Users can reliably pipe shtym output to other commands or redirect it to files</li> <li>Copy-paste friendliness: Humans can directly use the stdout content without manual cleanup</li> </ul>"},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/#implications","title":"Implications","text":""},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/#positive-implications","title":"Positive Implications","text":"<ul> <li>Clean integration with Unix pipes and redirections</li> <li>AI coding agents can parse output without special handling</li> <li>Predictable behavior: stdout always contains exactly the summary</li> <li>No dependency on terminal capabilities or detection logic</li> <li>Users can easily capture only the summary: <code>command | shtym &gt; summary.txt</code></li> </ul>"},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/#concerns","title":"Concerns","text":"<ul> <li>Users won't see progress indicators in stdout (mitigation: use stderr for progress)</li> <li>Debugging information must go to stderr (mitigation: implement proper logging levels)</li> <li>May feel \"bare\" compared to modern CLI tools with rich formatting (mitigation: this is intentional for our use case)</li> </ul>"},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/#alternatives","title":"Alternatives","text":""},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/#rich-cli-with-formatting-controls","title":"Rich CLI with formatting controls","text":"<p>Using libraries like rich or colorama to provide formatted output with an option to disable it.</p> <ul> <li>Pros: Better visual experience for interactive use, can show progress elegantly</li> <li>Cons: Adds complexity, requires terminal detection, users must remember to use <code>--plain</code> flags, increases dependencies</li> <li>Reason for rejection: Goes against the core philosophy of simplicity and context reduction</li> </ul>"},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/#json-output-mode","title":"JSON output mode","text":"<p>Wrapping the summary in JSON with metadata fields.</p> <ul> <li>Pros: Machine-readable, can include metadata</li> <li>Cons: Increases output size, requires parsing, defeats context reduction purpose</li> <li>Reason for rejection: Contradicts the goal of minimal context; metadata can go to stderr if needed</li> </ul>"},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/#mixed-output-with-delimiters","title":"Mixed output with delimiters","text":"<p>Using special delimiters to separate summary from other information in stdout.</p> <ul> <li>Pros: Can include both summary and metadata in one stream</li> <li>Cons: Users must parse delimiters, still increases output size, fragile</li> <li>Reason for rejection: Adds unnecessary complexity and increases context size</li> </ul>"},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/#future-direction","title":"Future Direction","text":"<p>This decision is foundational and unlikely to change. However, we should:</p> <ul> <li>Implement proper stderr logging for progress and debug information</li> <li>Consider environment variables or config files for verbosity control (affecting stderr only)</li> <li>Document this design choice clearly for users and contributors</li> <li>Revisit if a compelling use case emerges that requires stdout metadata (trigger: multiple user requests for structured output)</li> </ul>"},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/#references","title":"References","text":"<ul> <li>Unix Philosophy: \"Write programs that do one thing and do it well\"</li> </ul>"},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/#amendment-2025-11-21","title":"Amendment (2025-11-21)","text":""},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/#what-changed","title":"What Changed","text":"<p>The original ADR assumed pipe-based usage where users pipe command output into shtym:</p> <ul> <li>Original assumption: <code>pytest tests/ | stym</code></li> <li>Amended to: <code>stym run pytest tests/</code> (wrapper pattern with subcommand)</li> </ul> <p>The core principle of keeping stdout clean remains unchanged. Only the invocation pattern has changed from pipe-based to wrapper-based.</p>"},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/#reason-for-amendment","title":"Reason for Amendment","text":"<p>Exit code inheritance is a critical requirement for development workflows. When tests fail, build commands error, or linters find issues, the command must exit with a non-zero status to integrate properly with CI/CD pipelines and developer scripts.</p> <p>Technical constraint:</p> <ul> <li>Pipe pattern (<code>command | stym</code>): The wrapper process cannot access the exit code of the piped command. The pipeline's exit code would always be shtym's exit code, losing the original command's status.</li> <li>Wrapper pattern (<code>stym run command args</code>): The wrapper can execute the command as a subprocess, capture its exit code, and propagate it via <code>sys.exit(child_exit_code)</code>.</li> </ul> <p>Unix precedent: Standard Unix wrapper commands (sudo, timeout, time) all inherit their child process exit codes. This is the established pattern for command wrappers.</p>"},{"location":"adr/0001-keep-stdout-clean-for-ai-and-human-consumption/#impact-on-original-adr","title":"Impact on Original ADR","text":"<p>Unchanged:</p> <ul> <li>Clean stdout principle: stdout still contains only the primary data (command output or AI summary)</li> <li>stderr for metadata: progress indicators and errors still go to stderr</li> <li>Composability: shtym output can still be piped to other commands (<code>stym run pytest | grep ERROR</code>)</li> <li>Unix philosophy: still doing one thing well with clean interfaces</li> </ul> <p>Changed:</p> <ul> <li>Invocation pattern: wrapper style instead of pipe style</li> <li>Data flow: shtym executes the command rather than reading from stdin</li> <li>Exit code: properly inherited from child process</li> </ul> <p>Compatibility note: The wrapper pattern is more composable than originally described. Users can still pipe shtym's output:</p> <pre><code>stym run pytest tests/ | grep \"FAILED\"\nstym run npm test | tee test-output.txt\n</code></pre> <p>The clean stdout principle enables these compositions while also preserving exit codes.</p>"},{"location":"adr/0002-use-argparse-for-cli-implementation/","title":"ADR 0002: Use argparse for CLI Implementation","text":""},{"location":"adr/0002-use-argparse-for-cli-implementation/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/0002-use-argparse-for-cli-implementation/#date","title":"Date","text":"<p>2025-11-20</p>"},{"location":"adr/0002-use-argparse-for-cli-implementation/#context","title":"Context","text":"<p>Following ADR-0001's decision to keep stdout clean, shtym needs a CLI argument parsing library. Python offers several options including the standard library's argparse, and third-party libraries like click and typer that provide rich terminal UI features.</p> <p>The tool's design philosophy emphasizes simplicity and minimal dependencies. Since shtym's primary function is piping input through AI summarization, the CLI surface area is intentionally small. Complex interactive features, auto-completion, or rich formatting would not align with the clean stdout principle established in ADR-0001.</p>"},{"location":"adr/0002-use-argparse-for-cli-implementation/#decision","title":"Decision","text":"<p>Use argparse from Python's standard library for CLI argument parsing.</p>"},{"location":"adr/0002-use-argparse-for-cli-implementation/#rationale","title":"Rationale","text":"<ul> <li>Zero external dependencies: argparse is part of Python's standard library, requiring no additional packages</li> <li>Sufficient functionality: Provides all necessary features (flags, options, help text, subcommands if needed)</li> <li>No rich UI features: Lack of built-in colors, prompts, and progress bars aligns with ADR-0001's philosophy</li> <li>Simplicity: Straightforward API without magic decorators or complex abstractions</li> <li>Stability: Part of the standard library with guaranteed long-term support</li> <li>Predictability: Well-documented behavior, no surprises from framework-specific conventions</li> </ul>"},{"location":"adr/0002-use-argparse-for-cli-implementation/#implications","title":"Implications","text":""},{"location":"adr/0002-use-argparse-for-cli-implementation/#positive-implications","title":"Positive Implications","text":"<ul> <li>No dependency management overhead</li> <li>Works out-of-the-box in any Python environment</li> <li>Lightweight CLI with minimal overhead</li> <li>Clear, explicit argument definitions without decorators</li> <li>Easy to test with standard mocking techniques</li> <li>Reduces package size and installation complexity</li> </ul>"},{"location":"adr/0002-use-argparse-for-cli-implementation/#concerns","title":"Concerns","text":"<ul> <li>Manual help text formatting (mitigation: acceptable trade-off for simplicity)</li> <li>No built-in shell completion (mitigation: can add via argcomplete if needed, but likely unnecessary for our simple CLI)</li> <li>More verbose than decorator-based frameworks (mitigation: explicit is better than implicit, aids readability)</li> </ul>"},{"location":"adr/0002-use-argparse-for-cli-implementation/#alternatives","title":"Alternatives","text":""},{"location":"adr/0002-use-argparse-for-cli-implementation/#click","title":"Click","text":"<p>A popular third-party CLI framework with decorator-based API.</p> <ul> <li>Pros: Elegant decorator syntax, rich ecosystem, wide adoption</li> <li>Cons: External dependency, includes rich terminal features we don't need (colors, prompts, progress bars), adds framework magic</li> <li>Reason for rejection: Adds unnecessary dependency and features that contradict ADR-0001's clean output philosophy</li> </ul>"},{"location":"adr/0002-use-argparse-for-cli-implementation/#typer","title":"Typer","text":"<p>Modern CLI framework built on top of Click with type hints.</p> <ul> <li>Pros: Type-safe, modern Python syntax, automatic help generation from type hints</li> <li>Cons: External dependency, even heavier than Click (depends on Click + rich), includes rich terminal UI we don't want</li> <li>Reason for rejection: Same concerns as Click, plus additional dependency weight and unwanted UI features</li> </ul>"},{"location":"adr/0002-use-argparse-for-cli-implementation/#python-fire","title":"Python Fire","text":"<p>Automatically generates CLIs from Python objects.</p> <ul> <li>Pros: Zero boilerplate, automatic CLI generation</li> <li>Cons: External dependency, magic behavior, less explicit control, harder to understand CLI structure</li> <li>Reason for rejection: Too much magic, lack of explicit control contradicts our simplicity goal</li> </ul>"},{"location":"adr/0002-use-argparse-for-cli-implementation/#future-direction","title":"Future Direction","text":"<p>This decision is expected to remain stable. Potential triggers for revisiting:</p> <ul> <li>Complex subcommand structure emerges: If shtym grows to require many subcommands with nested options, a richer framework might become beneficial (unlikely given the tool's focused scope)</li> <li>Shell completion becomes essential: If users strongly request shell completion, we could add argcomplete as an optional dependency</li> <li>Standard library evolution: If argparse gains significant improvements or a better alternative enters the standard library</li> </ul> <p>For now, argparse provides everything needed for shtym's straightforward CLI.</p>"},{"location":"adr/0002-use-argparse-for-cli-implementation/#references","title":"References","text":"<ul> <li>ADR-0001: Keep stdout Clean for AI and Human Consumption</li> <li>Python argparse documentation</li> <li>PEP 389: argparse - New Command Line Parsing Module</li> </ul>"},{"location":"adr/0003-adopt-layered-architecture/","title":"ADR 0003: Adopt Layered Architecture","text":""},{"location":"adr/0003-adopt-layered-architecture/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/0003-adopt-layered-architecture/#date","title":"Date","text":"<p>2025-11-21</p>"},{"location":"adr/0003-adopt-layered-architecture/#context","title":"Context","text":"<p>Shtym needs to implement subprocess execution and output handling for pass-through mode (Issue #3), with future expansion to LLM integration. The current flat module structure (cli.py, core.py) lacks clear separation of concerns, which will make future LLM integration and testing more difficult.</p> <p>A clear architectural pattern is needed to:</p> <ul> <li>Separate I/O concerns from business logic</li> <li>Enable easy testing without actual I/O operations</li> <li>Provide clear boundaries for future LLM integration</li> <li>Maintain code organization as functionality grows</li> </ul>"},{"location":"adr/0003-adopt-layered-architecture/#decision","title":"Decision","text":"<p>Adopt a layered architecture with four distinct layers:</p> <pre><code>src/shtym/\n\u251c\u2500\u2500 _version.py           # Version constant\n\u251c\u2500\u2500 __init__.py           # Package exports\n\u251c\u2500\u2500 cli.py                # Presentation Layer\n\u251c\u2500\u2500 application.py        # Application Layer\n\u251c\u2500\u2500 domain.py             # Domain Layer\n\u2514\u2500\u2500 infrastructure/       # Infrastructure Layer\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 stdio.py          # stdout writing\n</code></pre> <p>Layer responsibilities:</p> <ul> <li>Presentation Layer (cli.py): CLI argument parsing, error handling, user-facing messages</li> <li>Application Layer (application.py): Orchestration of domain and infrastructure components</li> <li>Domain Layer (domain.py): Core text processing logic (pass-through, future LLM summarization)</li> <li>Infrastructure Layer (infrastructure/): External I/O operations (stdio, future LLM API clients)</li> </ul>"},{"location":"adr/0003-adopt-layered-architecture/#rationale","title":"Rationale","text":"<ul> <li>Testability: Each layer can be tested independently. Infrastructure I/O can be mocked in unit tests</li> <li>Separation of concerns: Clear boundaries between user interface, orchestration, business logic, and external dependencies</li> <li>Future LLM integration: LLM clients will naturally fit in infrastructure layer alongside stdio</li> <li>Maintainability: Changes to I/O mechanisms or CLI don't affect business logic</li> <li>Standard pattern: Layered architecture is well-understood and documented</li> <li>Lightweight: Only four layers, avoiding over-engineering for small project</li> </ul>"},{"location":"adr/0003-adopt-layered-architecture/#implications","title":"Implications","text":""},{"location":"adr/0003-adopt-layered-architecture/#positive-implications","title":"Positive Implications","text":"<ul> <li>Unit tests can focus on pure logic without I/O</li> <li>Infrastructure components are swappable (e.g., different LLM providers)</li> <li>Clear mental model for future contributors</li> <li>Dependency flow is explicit (presentation \u2192 application \u2192 domain \u2190 infrastructure)</li> <li>Easy to add new infrastructure adapters (file I/O, network, etc.)</li> </ul>"},{"location":"adr/0003-adopt-layered-architecture/#concerns","title":"Concerns","text":"<ul> <li>Slightly more files than flat structure (mitigation: only 3-4 additional modules, manageable)</li> <li>Requires discipline to maintain layer boundaries (mitigation: documented in ADR, enforced in code review)</li> <li>Possible over-engineering for simple pass-through (mitigation: pattern pays off immediately when LLM integration begins)</li> </ul>"},{"location":"adr/0003-adopt-layered-architecture/#alternatives","title":"Alternatives","text":""},{"location":"adr/0003-adopt-layered-architecture/#flat-module-structure","title":"Flat Module Structure","text":"<p>Keep current flat structure with cli.py and core.py.</p> <ul> <li>Pros: Fewer files, simpler initial setup</li> <li>Cons: Tight coupling between I/O and logic, hard to test, unclear where to add LLM clients</li> <li>Reason for rejection: Technical debt accumulates quickly; refactoring later is harder than starting with clear structure</li> </ul>"},{"location":"adr/0003-adopt-layered-architecture/#hexagonal-architecture-ports-and-adapters","title":"Hexagonal Architecture (Ports and Adapters)","text":"<p>Use ports (interfaces) and adapters pattern.</p> <ul> <li>Pros: Maximum flexibility, very testable, clear boundaries</li> <li>Cons: More abstract, requires interfaces/protocols for every boundary, overhead for small project</li> <li>Reason for rejection: Too much ceremony for current needs; layered architecture provides similar benefits with less complexity</li> </ul>"},{"location":"adr/0003-adopt-layered-architecture/#mvc-pattern","title":"MVC Pattern","text":"<p>Adopt Model-View-Controller pattern.</p> <ul> <li>Pros: Well-known web pattern</li> <li>Cons: Designed for UI interactions, not for CLI command wrapper tools; controller/view distinction unclear for CLI</li> <li>Reason for rejection: Not a natural fit for CLI command wrapper tools</li> </ul>"},{"location":"adr/0003-adopt-layered-architecture/#future-direction","title":"Future Direction","text":"<p>The layered architecture should remain stable through LLM integration. Potential triggers for revisiting:</p> <ul> <li>Infrastructure layer grows too large: If we add many infrastructure adapters (multiple LLM providers, various I/O sources), consider splitting into subdirectories (infrastructure/llm/, infrastructure/io/)</li> <li>Cross-cutting concerns emerge: If we need logging, metrics, or tracing across all layers, consider aspect-oriented patterns or middleware</li> <li>Domain logic becomes complex: If text processing logic grows significantly, consider splitting domain.py into multiple modules or introducing domain-driven design patterns</li> </ul> <p>For now, this lightweight four-layer structure provides the right balance of organization and simplicity.</p>"},{"location":"adr/0003-adopt-layered-architecture/#references","title":"References","text":"<ul> <li>Issue #3: Implement basic pass-through mode</li> <li>ADR-0001: Keep stdout Clean for AI and Human Consumption</li> <li>Layered Architecture Pattern</li> </ul>"},{"location":"adr/0004-do-not-implement-stdin-pipe-mode/","title":"ADR 0004: Do Not Implement Stdin Pipe Mode","text":""},{"location":"adr/0004-do-not-implement-stdin-pipe-mode/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/0004-do-not-implement-stdin-pipe-mode/#date","title":"Date","text":"<p>2025-11-21</p>"},{"location":"adr/0004-do-not-implement-stdin-pipe-mode/#context","title":"Context","text":"<p>When designing shtym's invocation pattern, there are two common approaches for wrapping command-line tools:</p> <ol> <li>Pipe mode: <code>command | shtym</code> - reads from stdin, writes to stdout</li> <li>Wrapper mode: <code>shtym command args</code> - executes command as subprocess</li> </ol> <p>The pipe mode (<code>command | shtym</code>) appears natural for Unix pipeline tools and aligns with the \"filter\" concept. However, this approach has a fundamental technical limitation that conflicts with critical development workflow requirements.</p>"},{"location":"adr/0004-do-not-implement-stdin-pipe-mode/#decision","title":"Decision","text":"<p>Do NOT implement stdin pipe mode (<code>command | shtym</code>). Shtym will exclusively use wrapper mode (<code>shtym command args</code>).</p>"},{"location":"adr/0004-do-not-implement-stdin-pipe-mode/#rationale","title":"Rationale","text":"<p>Technical constraint - Exit code inheritance:</p> <p>In development workflows, exit codes are critical for:</p> <ul> <li>CI/CD pipeline control (fail on test failures)</li> <li>Build automation (stop on compilation errors)</li> <li>Developer scripts (conditional execution based on success/failure)</li> <li>Shell scripting (<code>set -e</code> to fail on errors)</li> </ul> <p>Pipe mode limitation:</p> <pre><code>pytest tests/ | stym\necho $?  # Always returns stym's exit code, not pytest's\n</code></pre> <p>When using pipes, the shell pipeline's exit code is determined by the last command (<code>stym</code>), not the piped command (<code>pytest</code>). There is no standard mechanism for <code>stym</code> to access <code>pytest</code>'s exit code in this scenario.</p> <p>While <code>$PIPESTATUS</code> or <code>set -o pipefail</code> can provide workarounds, they:</p> <ul> <li>Require users to remember special shell features</li> <li>Don't work consistently across shells (bash vs zsh vs fish)</li> <li>Add cognitive overhead and documentation burden</li> <li>Break the principle of least surprise</li> </ul> <p>Wrapper mode advantage:</p> <pre><code>stym run pytest tests/\necho $?  # Returns pytest's exit code\n</code></pre> <p>Wrapper mode executes the command as a subprocess, captures its exit code via <code>subprocess.run()</code>, and propagates it via <code>sys.exit(child_returncode)</code>. This works reliably and requires no special user knowledge.</p> <p>Unix precedent:</p> <p>Standard Unix wrapper commands use this pattern:</p> <ul> <li><code>sudo command</code> - inherits exit code</li> <li><code>timeout command</code> - inherits exit code</li> <li><code>time command</code> - inherits exit code</li> <li><code>nice command</code> - inherits exit code</li> </ul> <p>Composability is preserved:</p> <p>Users can still pipe shtym's output:</p> <pre><code>stym run pytest tests/ | grep \"FAILED\"\nstym run npm test | tee output.txt\n</code></pre> <p>The wrapper pattern doesn't sacrifice composability; it enhances it by adding exit code reliability.</p>"},{"location":"adr/0004-do-not-implement-stdin-pipe-mode/#implications","title":"Implications","text":""},{"location":"adr/0004-do-not-implement-stdin-pipe-mode/#positive-implications","title":"Positive Implications","text":"<ul> <li>Exit codes work correctly without special shell configuration</li> <li>Matches Unix wrapper command conventions</li> <li>No documentation needed for <code>$PIPESTATUS</code> workarounds</li> <li>Works identically across all shells</li> <li>Integrates seamlessly with CI/CD systems</li> <li>Composable with pipes while preserving exit codes</li> </ul>"},{"location":"adr/0004-do-not-implement-stdin-pipe-mode/#concerns","title":"Concerns","text":"<ul> <li>Deviates from traditional \"filter\" mental model (mitigation: wrapper is more accurate for our use case)</li> <li>Users familiar with <code>command | filter</code> pattern may expect pipe mode (mitigation: clear documentation and error messages)</li> <li>Cannot process pre-existing piped input like <code>cat file | stym</code> (mitigation: not a target use case; use <code>stym run cat file</code> instead)</li> </ul>"},{"location":"adr/0004-do-not-implement-stdin-pipe-mode/#alternatives","title":"Alternatives","text":""},{"location":"adr/0004-do-not-implement-stdin-pipe-mode/#implement-pipe-mode-only","title":"Implement Pipe Mode Only","text":"<p>Use <code>command | stym</code> pattern exclusively.</p> <ul> <li>Pros: Familiar Unix filter pattern, simple mental model</li> <li>Cons: Cannot inherit exit codes, breaks CI/CD workflows, requires shell-specific workarounds</li> <li>Reason for rejection: Exit code inheritance is non-negotiable for development tool integration</li> </ul>"},{"location":"adr/0004-do-not-implement-stdin-pipe-mode/#implement-both-modes","title":"Implement Both Modes","text":"<p>Support both <code>command | stym</code> and <code>stym run command</code> with auto-detection.</p> <ul> <li>Pros: Maximum flexibility, supports both use cases</li> <li>Cons: Complex implementation, confusing UX (which mode is active?), pipe mode still can't solve exit code problem, doubles testing surface area</li> <li>Reason for rejection: Pipe mode provides no benefits over wrapper mode while adding complexity</li> </ul>"},{"location":"adr/0004-do-not-implement-stdin-pipe-mode/#use-special-flags-for-exit-code","title":"Use Special Flags for Exit Code","text":"<p>Implement pipe mode with <code>--exit-code=FILE</code> to write exit code separately.</p> <ul> <li>Pros: Solves exit code problem while keeping pipe mode</li> <li>Cons: Extremely awkward UX, requires temp files, fragile, no Unix precedent for this pattern</li> <li>Reason for rejection: Trading one problem for multiple worse problems</li> </ul>"},{"location":"adr/0004-do-not-implement-stdin-pipe-mode/#future-direction","title":"Future Direction","text":"<p>This decision is expected to remain stable. Potential triggers for revisiting:</p> <ul> <li>New shell features: If future shells provide standard mechanisms for filters to access piped command exit codes (unlikely; would break 50 years of Unix convention)</li> <li>User demand: If significant user feedback requests pipe mode despite exit code limitations (would require clear documentation of the tradeoff)</li> </ul> <p>For now, wrapper mode provides all benefits of pipe mode plus exit code inheritance, making it strictly superior for our use case.</p>"},{"location":"adr/0004-do-not-implement-stdin-pipe-mode/#references","title":"References","text":"<ul> <li>ADR-0001: Keep stdout Clean for AI and Human Consumption (Amended to use wrapper pattern)</li> <li>ADR-0005: Inherit Child Process Exit Codes</li> <li>Unix Pipe Documentation</li> </ul>"},{"location":"adr/0005-inherit-child-process-exit-codes/","title":"ADR 0005: Inherit Child Process Exit Codes","text":""},{"location":"adr/0005-inherit-child-process-exit-codes/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/0005-inherit-child-process-exit-codes/#date","title":"Date","text":"<p>2025-11-21</p>"},{"location":"adr/0005-inherit-child-process-exit-codes/#context","title":"Context","text":"<p>Shtym wraps command execution to filter or summarize output. As a command wrapper, it must decide how to handle the exit code (return code) of the child process it executes.</p> <p>Exit codes are fundamental to Unix command composition and automation:</p> <ul> <li>Exit code 0 indicates success</li> <li>Non-zero exit codes indicate failure (1-255)</li> <li>Scripts and CI/CD systems rely on exit codes for flow control</li> </ul> <p>When shtym executes <code>stym run pytest tests/</code>, there are several options for what exit code shtym itself should return.</p>"},{"location":"adr/0005-inherit-child-process-exit-codes/#decision","title":"Decision","text":"<p>Shtym MUST inherit and propagate the child process exit code exactly as-is. If the child process exits with code N, shtym exits with code N.</p> <p>Implementation:</p> <pre><code>result = subprocess.run(command, ...)\nsys.exit(result.returncode)  # Inherit exactly\n</code></pre>"},{"location":"adr/0005-inherit-child-process-exit-codes/#rationale","title":"Rationale","text":"<p>Critical for development workflows:</p> <p>Development commands have meaningful exit codes:</p> <ul> <li><code>pytest tests/</code> \u2192 0 if all tests pass, 1 if any test fails</li> <li><code>mypy src/</code> \u2192 0 if types are correct, 1 if type errors exist</li> <li><code>npm test</code> \u2192 0 if tests pass, non-zero if tests fail</li> <li><code>make build</code> \u2192 0 if build succeeds, non-zero if build fails</li> </ul> <p>If shtym returned its own exit code (e.g., always 0), it would break:</p> <ul> <li>CI/CD pipelines that fail builds on test failures</li> <li>Pre-commit hooks that block commits on linter errors</li> <li>Makefiles with conditional targets</li> <li>Shell scripts using <code>set -e</code> to fail fast</li> <li>Developer muscle memory (<code>command &amp;&amp; next-step</code>)</li> </ul> <p>Unix wrapper command convention:</p> <p>Standard Unix wrapper commands inherit child exit codes:</p> <pre><code>$ sudo false; echo $?\n1  # Inherits false's exit code\n\n$ timeout 5 false; echo $?\n1  # Inherits false's exit code\n\n$ time false; echo $?\n1  # Inherits false's exit code\n\n$ nice false; echo $?\n1  # Inherits false's exit code\n</code></pre> <p>This is established Unix convention for wrapper commands. Users expect wrappers to be transparent with respect to exit codes.</p> <p>Principle of least surprise:</p> <p>Developers expect:</p> <pre><code>stym run pytest tests/ &amp;&amp; echo \"Tests passed\"\n</code></pre> <p>To behave identically to:</p> <pre><code>pytest tests/ &amp;&amp; echo \"Tests passed\"\n</code></pre> <p>If shtym returned a different exit code, it would violate the principle of least surprise and be unusable in automated workflows.</p> <p>No loss of information:</p> <p>Shtym has no reason to override the child's exit code:</p> <ul> <li>Shtym's own errors (invalid arguments, etc.) prevent child execution, so there's no conflict</li> <li>Child execution errors are reflected in the child's exit code</li> <li>Shtym's purpose (filtering output) doesn't change the success/failure of the underlying command</li> </ul>"},{"location":"adr/0005-inherit-child-process-exit-codes/#implications","title":"Implications","text":""},{"location":"adr/0005-inherit-child-process-exit-codes/#positive-implications","title":"Positive Implications","text":"<ul> <li>Works seamlessly in CI/CD pipelines</li> <li>Integrates with shell scripting patterns (<code>&amp;&amp;</code>, <code>||</code>, <code>set -e</code>)</li> <li>Matches Unix wrapper command conventions</li> <li>Preserves all semantic information from child process</li> <li>No special documentation needed (behavior is intuitive)</li> <li>Enables <code>stym</code> to be used as a drop-in wrapper:</li> </ul> <pre><code>alias pytest='stym run pytest'  # Transparent wrapper\n</code></pre>"},{"location":"adr/0005-inherit-child-process-exit-codes/#concerns","title":"Concerns","text":"<ul> <li>If shtym itself encounters an error while the child succeeds, it cannot signal its own error via exit code (mitigation: errors that prevent output filtering prevent child execution, so no conflict exists; errors during filtering can be logged to stderr without affecting exit code)</li> <li>Edge case: if child exits with code 127 (command not found) vs shtym exits with 127, these are indistinguishable (mitigation: acceptable, as both indicate the same problem from the user's perspective)</li> </ul>"},{"location":"adr/0005-inherit-child-process-exit-codes/#alternatives","title":"Alternatives","text":""},{"location":"adr/0005-inherit-child-process-exit-codes/#return-shtyms-own-exit-code","title":"Return Shtym's Own Exit Code","text":"<p>Shtym returns 0 on successful execution (child ran successfully), regardless of child exit code.</p> <ul> <li>Pros: Distinguishes \"shtym worked\" from \"child worked\"</li> <li>Cons: Breaks all CI/CD integration, violates Unix conventions, unusable in automated workflows</li> <li>Reason for rejection: Makes shtym unsuitable for its primary use case (development command wrapping)</li> </ul>"},{"location":"adr/0005-inherit-child-process-exit-codes/#return-combined-exit-code","title":"Return Combined Exit Code","text":"<p>Use bitwise OR or custom encoding to combine shtym's status and child's exit code.</p> <ul> <li>Pros: Theoretically provides both pieces of information</li> <li>Cons: No Unix precedent, requires custom parsing, breaks standard exit code semantics (0 = success), confusing UX</li> <li>Reason for rejection: Violates Unix exit code conventions; no tool expects this pattern</li> </ul>"},{"location":"adr/0005-inherit-child-process-exit-codes/#exit-code-passthrough-flag","title":"Exit Code Passthrough Flag","text":"<p>Default to inheriting child exit code, but provide <code>--own-exit-code</code> flag to return shtym's status instead.</p> <ul> <li>Pros: Flexibility for edge cases</li> <li>Cons: Complicates UX, almost never useful, adds testing burden, invites misuse</li> <li>Reason for rejection: No identified use case for the flag; YAGNI (You Aren't Gonna Need It)</li> </ul>"},{"location":"adr/0005-inherit-child-process-exit-codes/#future-direction","title":"Future Direction","text":"<p>This decision is foundational and unlikely to change. Potential triggers for revisiting:</p> <ul> <li>Shtym processing errors: If shtym frequently encounters errors during output processing (e.g., LLM API failures) that need to be signaled separately from child failures (mitigation: log to stderr, use non-zero exit code only if child execution is prevented)</li> <li>Request for signal differentiation: If users need to distinguish \"child failed\" from \"shtym processing failed\" programmatically (mitigation: stderr logging, exit codes in different ranges, or status files if truly needed)</li> </ul> <p>For now, exact exit code inheritance provides the right semantics for a command wrapper and aligns with 50 years of Unix convention.</p>"},{"location":"adr/0005-inherit-child-process-exit-codes/#references","title":"References","text":"<ul> <li>ADR-0004: Do Not Implement Stdin Pipe Mode</li> <li>Exit Status (POSIX)</li> <li>Advanced Bash-Scripting Guide: Exit Codes With Special Meanings</li> </ul>"},{"location":"adr/0006-adopt-subcommand-architecture/","title":"ADR 0006: Adopt Subcommand Architecture","text":""},{"location":"adr/0006-adopt-subcommand-architecture/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/0006-adopt-subcommand-architecture/#date","title":"Date","text":"<p>2025-11-21</p>"},{"location":"adr/0006-adopt-subcommand-architecture/#context","title":"Context","text":"<p>Shtym's CLI was initially designed as a simple wrapper: <code>stym command args</code>. While this works for the basic pass-through functionality, future LLM integration will require additional commands for configuration and status checking.</p> <p>With a simple wrapper design, commands like <code>stym status</code> would be ambiguous:</p> <ul> <li>Does it execute a command named \"status\"?</li> <li>Does it show shtym's LLM status?</li> </ul> <p>This ambiguity becomes critical when planning for future features:</p> <ul> <li><code>stym status</code> - Check LLM connection, API key validity, usage quota</li> <li><code>stym config</code> - View or modify configuration (API keys, default model)</li> <li><code>stym models</code> - List available LLM models</li> </ul> <p>Without a clear subcommand structure, these would either:</p> <ol> <li>Require awkward flag syntax (<code>stym --status</code>, <code>stym --config</code>)</li> <li>Use reserved word lists (fragile, prevents wrapping commands with those names)</li> <li>Be impossible to implement without breaking changes</li> </ol> <p>This decision must be made before the first release because changing from <code>stym command</code> to <code>stym run command</code> later would be a breaking change for all users.</p>"},{"location":"adr/0006-adopt-subcommand-architecture/#decision","title":"Decision","text":"<p>Adopt a subcommand architecture using argparse subparsers:</p> <pre><code>stym run &lt;command&gt; [args...]     # Execute and filter command\nstym status                      # (Future) Check LLM status\nstym config [options]            # (Future) Manage configuration\nstym models                      # (Future) List available models\n</code></pre>"},{"location":"adr/0006-adopt-subcommand-architecture/#rationale","title":"Rationale","text":"<p>Extensibility for LLM integration:</p> <ul> <li>Clear namespace for shtym operations vs wrapped commands</li> <li>No ambiguity: <code>stym status</code> always means shtym's status, never a command named \"status\"</li> <li>Natural place for future subcommands without breaking changes</li> </ul> <p>Industry standard pattern:</p> <ul> <li><code>git commit</code>, <code>docker run</code>, <code>kubectl get</code> - all use subcommands</li> <li><code>gh pr create</code>, <code>cargo build</code> - modern CLIs follow this pattern</li> <li>Users familiar with these tools will immediately understand shtym's structure</li> </ul> <p>Prevents future breaking changes:</p> <ul> <li>Adding <code>stym status</code> later would break users running <code>stym status</code> expecting command execution</li> <li>Changing from <code>stym pytest</code> to <code>stym run pytest</code> would break all existing scripts and documentation</li> <li>Must decide now, before first release and user adoption</li> </ul> <p>Explicit is better than implicit:</p> <ul> <li><code>stym run pytest</code> clearly indicates \"run this command through shtym\"</li> <li>No magic reserved words or special parsing rules</li> <li>Easier to explain and document</li> </ul>"},{"location":"adr/0006-adopt-subcommand-architecture/#implications","title":"Implications","text":""},{"location":"adr/0006-adopt-subcommand-architecture/#positive-implications","title":"Positive Implications","text":"<ul> <li>Future-proof: can add <code>status</code>, <code>config</code>, <code>models</code> without breaking changes</li> <li>Clear separation: shtym commands vs wrapped commands</li> <li>Industry-standard pattern: familiar to users</li> <li>Enables rich CLI features: help for each subcommand, subcommand-specific options</li> <li>Testable: each subcommand can be tested independently</li> </ul>"},{"location":"adr/0006-adopt-subcommand-architecture/#concerns","title":"Concerns","text":"<ul> <li>Slightly more typing: <code>stym run pytest</code> vs <code>stym pytest</code> (mitigation: users can create shell aliases if desired: <code>alias sr='stym run'</code>)</li> <li>Breaking change from current development version (mitigation: no released version yet, no users affected)</li> </ul>"},{"location":"adr/0006-adopt-subcommand-architecture/#alternatives","title":"Alternatives","text":""},{"location":"adr/0006-adopt-subcommand-architecture/#simple-wrapper-no-subcommands","title":"Simple Wrapper (No Subcommands)","text":"<p>Keep <code>stym command args</code> pattern, add shtym operations as flags.</p> <ul> <li>Pros: Shortest syntax for command execution</li> <li>Cons: Awkward operation syntax (<code>stym --status</code>), limited extensibility, confusing help output</li> <li>Reason for rejection: Cannot support rich LLM features without awkward UX</li> </ul>"},{"location":"adr/0006-adopt-subcommand-architecture/#reserved-word-list","title":"Reserved Word List","text":"<p>Use <code>stym command</code> but treat certain words (status, config, etc.) as special.</p> <ul> <li>Pros: Short command execution syntax</li> <li>Cons: Cannot wrap commands named \"status\", \"config\", etc.; fragile (must maintain reserved list); confusing (\"why doesn't <code>stym status</code> work?\")</li> <li>Reason for rejection: Too many edge cases and user confusion</li> </ul>"},{"location":"adr/0006-adopt-subcommand-architecture/#auto-detection","title":"Auto-Detection","text":"<p>Detect whether argument is a shtym subcommand or external command.</p> <ul> <li>Pros: \"Smart\" behavior</li> <li>Cons: Magic behavior, ambiguity (what if someone creates a command named \"status\"?), hard to predict</li> <li>Reason for rejection: Explicit is better than implicit; too much magic</li> </ul>"},{"location":"adr/0006-adopt-subcommand-architecture/#future-direction","title":"Future Direction","text":"<p>This subcommand structure is foundational and should remain stable. Expected future subcommands:</p> <p>Planned:</p> <ul> <li><code>stym status</code> - LLM connection and usage status</li> <li><code>stym config</code> - Configuration management</li> <li><code>stym models</code> - List and manage available models</li> </ul> <p>Possible:</p> <ul> <li><code>stym history</code> - Command execution history</li> <li><code>stym explain</code> - Explain previous command output</li> <li><code>stym debug</code> - Debug mode with verbose logging</li> </ul> <p>Triggers for revisiting:</p> <ul> <li>If <code>stym run</code> becomes overwhelming majority of usage and other subcommands rarely used (could add <code>run</code> as default, but keep subcommand structure)</li> <li>If user feedback strongly prefers shorter syntax despite tradeoffs (could add shell completion to make <code>stym run</code> typing easier)</li> </ul> <p>For now, explicit subcommand architecture provides the best balance of clarity, extensibility, and future-proofing.</p>"},{"location":"adr/0006-adopt-subcommand-architecture/#references","title":"References","text":"<ul> <li>Issue #3: Implement basic pass-through mode</li> <li>ADR-0002: Use argparse for CLI Implementation</li> <li>Git Subcommands</li> </ul>"},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/","title":"ADR 0007: Introduce Filter Abstraction for Output Processing","text":""},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/#status","title":"Status","text":"<p>Accepted (Amended 2025-12-06)</p>"},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/#date","title":"Date","text":"<p>2025-11-21</p>"},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/#context","title":"Context","text":"<p>Shtym is implementing basic pass-through mode (Issue #3) with the understanding that LLM-based output filtering will be added in the near future. The initial implementation could directly pass subprocess output to stdout, but this approach would require significant refactoring when adding LLM integration.</p> <p>Key considerations:</p> <ul> <li>Current requirement: Pass subprocess output through unchanged</li> <li>Future requirement: Filter output through LLM for summarization</li> <li>Goal: Minimize code changes when adding LLM integration</li> <li>Constraint: Avoid over-engineering for hypothetical requirements</li> </ul> <p>The question is whether to introduce an abstraction layer now or refactor later when LLM integration is implemented.</p>"},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/#decision","title":"Decision","text":"<p>Introduce a <code>Filter</code> protocol and implement <code>PassThroughFilter</code> immediately, even though current behavior requires no transformation.</p> <p>Architecture:</p> <pre><code># Domain layer - Filter protocol\nclass Filter(Protocol):\n    def filter(self, text: str) -&gt; str: ...\n\n# Domain layer - PassThroughFilter implementation\nclass PassThroughFilter:\n    def filter(self, text: str) -&gt; str:\n        return text\n\n# Application layer - process_command uses Filter\ndef process_command(\n    command: list[str], text_filter: Filter\n) -&gt; ProcessedCommandResult:\n    result = run_command(command)\n    filtered_output = text_filter.filter(result.stdout)\n    return ProcessedCommandResult(filtered_output, result.returncode)\n\n# Presentation layer - CLI instantiates filter\ndef main() -&gt; None:\n    # ...\n    text_filter = PassThroughFilter()\n    result = process_command(args.command, text_filter)\n    write_stdout(result.filtered_output)\n</code></pre>"},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/#rationale","title":"Rationale","text":"<p>Minimizes future LLM integration changes:</p> <p>Adding LLM filtering will only require:</p> <ol> <li>Implementing <code>LLMFilter</code> class with <code>filter(text: str) -&gt; str</code> method</li> <li>Changing CLI to instantiate <code>LLMFilter</code> instead of <code>PassThroughFilter</code></li> <li>Adding LLM configuration (API keys, model selection, etc.)</li> </ol> <p>No changes needed to:</p> <ul> <li><code>process_command</code> function</li> <li>Test infrastructure</li> <li>Exit code handling</li> <li>Core command execution logic</li> </ul> <p>Follows Dependency Injection pattern:</p> <ul> <li><code>process_command</code> depends on abstraction (<code>Filter</code> protocol), not concrete implementation</li> <li>Makes testing trivial - can inject mock filters with predictable behavior</li> <li>Presentation layer controls which filter to use based on configuration</li> </ul> <p>Adheres to SOLID principles:</p> <ul> <li>Open/Closed: Can add new filter types without modifying <code>process_command</code></li> <li>Liskov Substitution: Any <code>Filter</code> implementation works identically from caller's perspective</li> <li>Dependency Inversion: Application layer depends on domain abstraction, not infrastructure implementation</li> </ul> <p>Minimal current overhead:</p> <p><code>PassThroughFilter</code> is trivial (~3 lines) and has zero performance impact. The abstraction adds clarity even for pass-through behavior: the code explicitly shows \"we're applying a filter, which happens to pass through unchanged\" rather than implicitly passing stdout.</p> <p>Clear extension point:</p> <p>The <code>Filter</code> protocol serves as documentation: \"This is where output transformation happens.\" Future developers immediately understand where LLM integration belongs.</p>"},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/#implications","title":"Implications","text":""},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/#positive-implications","title":"Positive Implications","text":"<ul> <li>Smooth LLM integration: Adding <code>LLMFilter</code> is straightforward - implement protocol and swap in CLI</li> <li>Testability: Can test filters independently; can test <code>process_command</code> with mock filters</li> <li>Flexibility: Easy to support multiple filter types (pass-through, LLM summarization, custom filters)</li> <li>Separation of concerns: Output transformation logic separated from command execution</li> <li>Type safety: Protocol provides IDE autocomplete and type checking</li> </ul>"},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/#concerns","title":"Concerns","text":"<ul> <li>Slight complexity increase: Adds abstraction layer for currently-trivial behavior (mitigation: abstraction is minimal and will pay off immediately when LLM integration begins)</li> <li>Indirection: One extra function call (<code>filter.filter()</code>) in execution path (mitigation: negligible performance impact, clarity benefit outweighs cost)</li> </ul>"},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/#alternatives","title":"Alternatives","text":""},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/#direct-stdout-pass-through","title":"Direct stdout Pass-Through","text":"<p>Pass subprocess stdout directly to <code>write_stdout</code> without filter abstraction.</p> <pre><code>def main() -&gt; None:\n    result = run_command(args.command)\n    write_stdout(result.stdout)\n    sys.exit(result.returncode)\n</code></pre> <ul> <li>Pros: Simplest possible implementation, zero abstraction overhead</li> <li>Cons: LLM integration requires modifying <code>run_command</code>, CLI logic, and all tests; no clear extension point</li> <li>Reason for rejection: Known future requirement (LLM integration) makes abstraction worthwhile immediately</li> </ul>"},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/#conditional-filter-application","title":"Conditional Filter Application","text":"<p>Check flag/config and conditionally apply filter.</p> <pre><code>def process_command(command, use_llm=False):\n    result = run_command(command)\n    if use_llm:\n        return llm_filter(result.stdout)\n    return result.stdout\n</code></pre> <ul> <li>Pros: No protocol needed, logic in one place</li> <li>Cons: Violates Open/Closed Principle; adding new filter types requires modifying <code>process_command</code>; harder to test</li> <li>Reason for rejection: Doesn't scale to multiple filter types; protocol-based approach is cleaner</li> </ul>"},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/#strategy-pattern-with-base-class","title":"Strategy Pattern with Base Class","text":"<p>Use abstract base class instead of Protocol.</p> <pre><code>class Filter(ABC):\n    @abstractmethod\n    def filter(self, text: str) -&gt; str: ...\n</code></pre> <ul> <li>Pros: Enforces implementation via ABC; similar to Protocol approach</li> <li>Cons: Requires inheritance; Protocol is more Pythonic for duck typing; heavier-weight</li> <li>Reason for rejection: Protocol provides same benefits with lighter syntax; aligns with Python typing best practices</li> </ul>"},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/#future-direction","title":"Future Direction","text":"<p>This abstraction should remain stable through LLM integration and beyond. Potential triggers for revisiting:</p> <ul> <li>Multiple transformation steps: If we need chaining (e.g., LLM summarization \u2192 Markdown formatting \u2192 Syntax highlighting), consider Composite Pattern or pipeline approach</li> <li>Streaming output: If we need to filter output as it streams (not batch), consider async generators or streaming protocols</li> <li>Context-aware filtering: If filters need access to command context (working directory, environment, history), consider enriching Filter protocol with context parameter</li> </ul> <p>For now, the simple <code>filter(text: str) -&gt; str</code> interface provides exactly what's needed for both pass-through and LLM summarization.</p>"},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/#references","title":"References","text":"<ul> <li>Issue #3: Implement basic pass-through mode</li> <li>ADR-0003: Adopt Layered Architecture</li> <li>Strategy Pattern</li> <li>Dependency Injection Principle</li> </ul>"},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/#amendment-2025-12-06","title":"Amendment (2025-12-06)","text":""},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/#what-changed","title":"What Changed","text":"<p>The term \"Filter\" used throughout this ADR has been renamed to \"Processor\":</p> <ul> <li><code>Filter</code> Protocol \u2192 <code>Processor</code> Protocol</li> <li><code>PassThroughFilter</code> \u2192 <code>PassThroughProcessor</code></li> <li><code>LLMFilter</code> \u2192 <code>LLMProcessor</code></li> </ul>"},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/#reason-for-amendment","title":"Reason for Amendment","text":"<p>The term \"Filter\" strongly implies reduction or removal (filtering out unwanted elements), but shtym's transformation capabilities extend beyond reduction to include expansion, conversion, and translation. \"Processor\" is a more neutral term that accurately represents the abstraction's purpose of transforming output in various ways.</p> <p>See ADR-0013 for detailed rationale.</p>"},{"location":"adr/0007-introduce-filter-abstraction-for-output-processing/#impact-on-original-adr","title":"Impact on Original ADR","text":"<p>Unchanged:</p> <ul> <li>The architectural decision to introduce an abstraction for output transformation remains valid</li> <li>The Strategy pattern and Dependency Injection principles still apply</li> <li>The benefits of testability, flexibility, and separation of concerns are unchanged</li> </ul> <p>Changed:</p> <ul> <li>Terminology only: \"Filter\" \u2192 \"Processor\" throughout the codebase</li> <li>No changes to the abstraction's interface or behavior</li> </ul>"},{"location":"adr/0008-introduce-llm-client-abstraction/","title":"ADR 0008: Introduce LLM Client Abstraction","text":""},{"location":"adr/0008-introduce-llm-client-abstraction/#status","title":"Status","text":"<p>Accepted (Amended 2025-12-08)</p>"},{"location":"adr/0008-introduce-llm-client-abstraction/#date","title":"Date","text":"<p>2025-12-02</p>"},{"location":"adr/0008-introduce-llm-client-abstraction/#context","title":"Context","text":"<p>Issue #5 introduces LLM-based filtering using Ollama. Future requirements include support for multiple LLM providers (OpenAI, Claude, etc.).</p> <p>Without abstraction, domain layer would directly depend on Ollama-specific APIs (<code>ollama.Client</code>, <code>Message</code>, <code>ResponseError</code>), making provider switching difficult and leaking infrastructure details into domain logic.</p>"},{"location":"adr/0008-introduce-llm-client-abstraction/#decision","title":"Decision","text":"<p>Introduce <code>LLMClient</code> protocol in domain layer and <code>OllamaLLMClient</code> in infrastructure layer.</p> <pre><code>domain/\n  llm_client.py     # LLMClient protocol\n  filter.py         # LLMFilter depends on LLMClient\n\ninfrastructure/\n  ollama_client.py  # OllamaLLMClient implements LLMClient\n</code></pre> <p>LLMFilter (domain) depends on LLMClient protocol (domain). OllamaLLMClient (infrastructure) implements the protocol. Application layer injects client into filter.</p>"},{"location":"adr/0008-introduce-llm-client-abstraction/#rationale","title":"Rationale","text":"<ul> <li>Dependency Inversion: Domain depends on abstraction, not concrete infrastructure</li> <li>Extensibility: Adding providers requires only new infrastructure implementation, zero domain changes</li> <li>Testing: Domain tests mock simple protocol; infrastructure tests verify real integrations</li> <li>Encapsulation: Provider-specific details (auth, message formats, errors) isolated in infrastructure</li> </ul>"},{"location":"adr/0008-introduce-llm-client-abstraction/#implications","title":"Implications","text":""},{"location":"adr/0008-introduce-llm-client-abstraction/#positive-implications","title":"Positive Implications","text":"<ul> <li>Adding new providers requires no domain changes</li> <li>Domain tests use simple mocks</li> <li>Provider complexity isolated in infrastructure</li> </ul>"},{"location":"adr/0008-introduce-llm-client-abstraction/#concerns","title":"Concerns","text":"<ul> <li>Adds one layer of indirection (mitigation: negligible performance impact)</li> <li>Simple interface may not expose advanced features (mitigation: extend protocol when needed)</li> </ul>"},{"location":"adr/0008-introduce-llm-client-abstraction/#alternatives","title":"Alternatives","text":""},{"location":"adr/0008-introduce-llm-client-abstraction/#ollama-direct-in-domain-layer","title":"Ollama Direct in Domain Layer","text":"<ul> <li>Cons: Domain depends on infrastructure; adding providers requires domain changes</li> <li>Reason for rejection: Multiple provider support is planned</li> </ul>"},{"location":"adr/0008-introduce-llm-client-abstraction/#strategy-pattern-with-provider-enum","title":"Strategy Pattern with Provider Enum","text":"<ul> <li>Cons: Violates Open/Closed; provider logic mixed in domain</li> <li>Reason for rejection: Doesn't scale</li> </ul>"},{"location":"adr/0008-introduce-llm-client-abstraction/#abstract-base-class","title":"Abstract Base Class","text":"<ul> <li>Cons: Requires inheritance; less flexible than Protocol</li> <li>Reason for rejection: Protocol is more Pythonic</li> </ul>"},{"location":"adr/0008-introduce-llm-client-abstraction/#references","title":"References","text":"<ul> <li>Issue #5: Add LLM-based filtering with Ollama</li> <li>ADR-0003: Adopt Layered Architecture</li> <li>ADR-0007: Introduce Filter Abstraction for Output Processing</li> <li>Dependency Inversion Principle</li> </ul>"},{"location":"adr/0008-introduce-llm-client-abstraction/#amendment-2025-12-08","title":"Amendment (2025-12-08)","text":""},{"location":"adr/0008-introduce-llm-client-abstraction/#what-changed","title":"What Changed","text":"<p>The LLMClient protocol placement described in the original ADR has been corrected.</p> <p>Original (incorrect): <pre><code>domain/\n  llm_client.py     # LLMClient protocol\n  filter.py         # LLMFilter depends on LLMClient\n\ninfrastructure/\n  ollama_client.py  # OllamaLLMClient implements LLMClient\n</code></pre></p> <p>Corrected: <pre><code>infrastructure/\n  processors/\n    llm_processor.py  # Contains both LLMClient protocol and LLMProcessor\n  llm_clients/\n    ollama_client.py  # OllamaLLMClient implements LLMClient\n</code></pre></p>"},{"location":"adr/0008-introduce-llm-client-abstraction/#reason-for-amendment","title":"Reason for Amendment","text":"<p>LLM is an infrastructure concern, not a domain concept.</p> <p>Domain layer represents business logic and core concepts. The domain concept is \"process command output,\" not \"process using LLM.\" LLM is a technical implementation detail of how processing happens.</p> <p>Correct architecture: - Domain layer: Processor protocol (abstraction for output processing) - Infrastructure layer: LLMClient protocol (abstraction within infrastructure for LLM provider independence) - Infrastructure layer: LLMProcessor (implements Processor, uses LLMClient) - Infrastructure layer: OllamaLLMClient (implements LLMClient)</p> <p>The LLMClient protocol serves as an internal abstraction within the infrastructure layer, enabling Dependency Inversion between LLMProcessor and specific LLM provider implementations (OllamaLLMClient, future OpenAI client, etc.).</p>"},{"location":"adr/0008-introduce-llm-client-abstraction/#impact-on-original-adr","title":"Impact on Original ADR","text":"<p>Unchanged: - The decision to introduce LLMClient abstraction remains valid - Dependency Inversion Principle still applies (within infrastructure layer) - Benefits of extensibility and testability are preserved</p> <p>Changed: - Layer placement: LLMClient is infrastructure abstraction, not domain abstraction - Domain layer remains ignorant of LLM details - LLMClient and LLMProcessor colocated in same module for cohesion</p>"},{"location":"adr/0009-silent-fallback-to-passthrough-filter/","title":"ADR 0009: Silent Fallback to PassThrough Filter on Model Unavailability","text":""},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#status","title":"Status","text":"<p>Accepted (Amended 2025-12-06)</p>"},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#date","title":"Date","text":"<p>2025-12-04</p>"},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#context","title":"Context","text":"<p>Issue #7 adds environment variable support for LLM model configuration (<code>SHTYM_LLM_SETTINGS__MODEL</code>).</p> <p>When the specified model is unavailable (not installed in Ollama, typo in model name, etc.), the application must decide whether to fail with an error or continue execution.</p> <p>Current implementation: <code>Application.create()</code> checks <code>OllamaLLMClient.is_available()</code>. If <code>False</code>, it silently falls back to <code>PassThroughFilter</code>.</p>"},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#decision","title":"Decision","text":"<p>When configured LLM model is unavailable, silently fall back to <code>PassThroughFilter</code> without warnings or errors.</p> <pre><code>@classmethod\ndef create(cls, command: list[str]) -&gt; \"Application\":\n    try:\n        llm_client = OllamaLLMClient.create()\n        if llm_client.is_available():\n            filter = LLMFilter(llm_client=llm_client)\n        else:\n            filter = PassThroughFilter()\n    except ImportError:\n        filter = PassThroughFilter()\n    return cls(command=command, filter=filter)\n</code></pre>"},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#rationale","title":"Rationale","text":"<ul> <li>Graceful degradation: User can continue work even when LLM unavailable</li> <li>Zero-configuration default: Works without Ollama installation</li> <li>No interruption: No manual intervention required when model missing</li> <li>Consistent behavior: Same fallback mechanism for all unavailability scenarios</li> </ul>"},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#implications","title":"Implications","text":""},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#positive-implications","title":"Positive Implications","text":"<ul> <li>Application never fails due to LLM configuration issues</li> <li>Smooth user experience in environments without LLM access</li> <li>No breaking changes when upgrading models or switching environments</li> </ul>"},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#concerns","title":"Concerns","text":"<ul> <li>Silent errors hide configuration mistakes (mitigation: add logging in future to warn when falling back to PassThroughFilter)</li> <li>Unexpected behavior when user expects LLM filtering but gets raw output (mitigation: add <code>--verbose</code> flag to show fallback notifications)</li> <li>Difficult debugging without indication that fallback occurred (mitigation: add <code>stym doctor</code> command to validate LLM configuration)</li> </ul>"},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#alternatives","title":"Alternatives","text":""},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#fail-with-error-on-model-unavailability","title":"Fail with Error on Model Unavailability","text":"<p>Behavior: Exit with error message when model unavailable.</p> <p>Pros: - User immediately aware of configuration issues - Explicit failure easier to debug than silent fallback</p> <p>Cons: - Breaks zero-configuration experience - Requires manual intervention for every configuration issue - Inconsistent with graceful degradation philosophy</p> <p>Reason for rejection: Prioritize availability over strict validation in initial implementation.</p>"},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#warn-but-continue","title":"Warn but Continue","text":"<p>Behavior: Print warning to stderr, then fall back to PassThroughFilter.</p> <p>Pros: - User notified of fallback - Still allows continued execution</p> <p>Cons: - Warning noise in logs/CI environments - May be ignored or filtered out - Complicates testing (need to assert stderr)</p> <p>Reason for rejection: Defer notification mechanism to future enhancement. Current simple behavior easier to test and reason about.</p>"},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#future-direction","title":"Future Direction","text":""},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#follow-up-actions","title":"Follow-up Actions","text":"<ol> <li>Add structured logging: Implement logging framework to warn when falling back to PassThroughFilter</li> <li>Add verbose mode: Implement <code>--verbose</code> flag to show fallback notifications</li> <li>Add diagnostic command: Implement <code>stym doctor</code> command to validate LLM configuration and report issues</li> <li>Add strict mode: Implement <code>--strict-llm</code> flag to fail fast when model unavailable</li> </ol>"},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#triggers-for-revisiting","title":"Triggers for Revisiting","text":"<p>This decision should be reconsidered when:</p> <ul> <li>Users report confusion about unexpected passthrough behavior</li> <li>Multiple bug reports about model configuration typos go unnoticed</li> <li>CI/CD pipelines require explicit validation of LLM availability</li> <li>Production deployments need guaranteed LLM filtering (no silent fallback)</li> </ul>"},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#references","title":"References","text":"<ul> <li>Issue #7: Allow LLM model configuration via environment variable</li> <li>ADR-0007: Introduce Filter Abstraction for Output Processing</li> <li>ADR-0008: Introduce LLM Client Abstraction</li> </ul>"},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#amendment-2025-12-06","title":"Amendment (2025-12-06)","text":""},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#what-changed","title":"What Changed","text":"<p>The term \"Filter\" used throughout this ADR has been renamed to \"Processor\":</p> <ul> <li>\"PassThrough Filter\" \u2192 \"PassThrough Processor\"</li> <li>\"LLMFilter\" \u2192 \"LLMProcessor\"</li> </ul>"},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#reason-for-amendment","title":"Reason for Amendment","text":"<p>The term \"Filter\" strongly implies reduction or removal, but shtym's transformation capabilities extend beyond reduction. \"Processor\" is a more neutral and accurate term for output transformation.</p> <p>See ADR-0013 for detailed rationale.</p>"},{"location":"adr/0009-silent-fallback-to-passthrough-filter/#impact-on-original-adr","title":"Impact on Original ADR","text":"<p>Unchanged:</p> <ul> <li>The decision to silently fall back when model is unavailable remains valid</li> <li>The graceful degradation philosophy is unchanged</li> <li>Future direction (logging, verbose mode, doctor command) remains applicable</li> </ul> <p>Changed:</p> <ul> <li>Terminology only: \"Filter\" \u2192 \"Processor\" throughout the codebase and documentation</li> </ul>"},{"location":"adr/0010-introduce-profile-as-core-domain-object/","title":"ADR 0010: Introduce Profile as Core Domain Object","text":""},{"location":"adr/0010-introduce-profile-as-core-domain-object/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/0010-introduce-profile-as-core-domain-object/#date","title":"Date","text":"<p>2025-12-05</p>"},{"location":"adr/0010-introduce-profile-as-core-domain-object/#context","title":"Context","text":"<p>The application transforms command output using LLM. Users need a way to configure transformation behavior (prompt style, output format, LLM settings).</p> <p>Currently, the application makes filter selection decisions internally without user control.</p>"},{"location":"adr/0010-introduce-profile-as-core-domain-object/#decision","title":"Decision","text":"<p>Introduce \"Profile\" as a core domain concept. A profile represents a named configuration for output transformation.</p>"},{"location":"adr/0010-introduce-profile-as-core-domain-object/#rationale","title":"Rationale","text":"<ul> <li>Users think in terms of transformation modes (\"summarize briefly\", \"explain in detail\", \"extract errors\"), not technical filter implementations</li> <li>Configuration naturally groups into reusable named profiles</li> <li>Profiles provide a stable user-facing abstraction independent of implementation changes</li> </ul>"},{"location":"adr/0010-introduce-profile-as-core-domain-object/#implications","title":"Implications","text":""},{"location":"adr/0010-introduce-profile-as-core-domain-object/#positive-implications","title":"Positive Implications","text":"<ul> <li>Clear vocabulary for discussing transformation configuration</li> <li>Natural extension point for user-defined transformation behaviors</li> <li>Decouples user intent from technical implementation</li> </ul>"},{"location":"adr/0010-introduce-profile-as-core-domain-object/#concerns","title":"Concerns","text":"<ul> <li>Additional abstraction layer (mitigation: profiles match user mental model, not artificial complexity)</li> </ul>"},{"location":"adr/0010-introduce-profile-as-core-domain-object/#alternatives","title":"Alternatives","text":""},{"location":"adr/0010-introduce-profile-as-core-domain-object/#direct-filter-configuration","title":"Direct Filter Configuration","text":"<p>Let users directly configure filters and LLM clients.</p> <p>Pros: No intermediate abstraction</p> <p>Cons: Exposes implementation details; users must understand filter architecture; configuration becomes fragile as implementation evolves</p> <p>Reason for rejection: User-facing API should reflect user concepts, not internal architecture</p>"},{"location":"adr/0010-introduce-profile-as-core-domain-object/#references","title":"References","text":"<ul> <li>Issue #10: Introduce default profile concept</li> <li>ADR-0003: Adopt Layered Architecture</li> </ul>"},{"location":"adr/0011-silent-fallback-on-profile-not-found/","title":"ADR 0011: Silent Fallback on Profile Not Found","text":""},{"location":"adr/0011-silent-fallback-on-profile-not-found/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/0011-silent-fallback-on-profile-not-found/#date","title":"Date","text":"<p>2025-12-05</p>"},{"location":"adr/0011-silent-fallback-on-profile-not-found/#context","title":"Context","text":"<p>When a user specifies a nonexistent profile name, the application must decide whether to fail with an error or continue execution.</p> <p>ADR-0009 established a pattern of silent fallback to PassThroughFilter when LLM models are unavailable.</p>"},{"location":"adr/0011-silent-fallback-on-profile-not-found/#decision","title":"Decision","text":"<p>When the requested profile does not exist, silently fall back to PassThroughFilter without warnings or errors.</p>"},{"location":"adr/0011-silent-fallback-on-profile-not-found/#rationale","title":"Rationale","text":"<ul> <li>Consistency: Follows ADR-0009's silent fallback pattern for unavailable resources</li> <li>Graceful degradation: User can continue work even with profile misconfiguration</li> <li>Zero-configuration default: Application works without profile setup</li> </ul>"},{"location":"adr/0011-silent-fallback-on-profile-not-found/#implications","title":"Implications","text":""},{"location":"adr/0011-silent-fallback-on-profile-not-found/#positive-implications","title":"Positive Implications","text":"<ul> <li>Application never fails due to profile configuration issues</li> <li>Consistent behavior across all unavailability scenarios (LLM, profiles)</li> </ul>"},{"location":"adr/0011-silent-fallback-on-profile-not-found/#concerns","title":"Concerns","text":"<ul> <li>Silent errors hide configuration mistakes (mitigation: future <code>stym doctor</code> command will validate profile configuration)</li> <li>Unexpected behavior when user expects specific profile (mitigation: future <code>--verbose</code> flag will show fallback notifications)</li> </ul>"},{"location":"adr/0011-silent-fallback-on-profile-not-found/#alternatives","title":"Alternatives","text":""},{"location":"adr/0011-silent-fallback-on-profile-not-found/#fail-with-error","title":"Fail with Error","text":"<p>Exit with error message when profile not found.</p> <p>Pros: User immediately aware of configuration issues</p> <p>Cons: Breaks zero-configuration experience; requires manual intervention</p> <p>Reason for rejection: Inconsistent with ADR-0009 graceful degradation philosophy</p>"},{"location":"adr/0011-silent-fallback-on-profile-not-found/#warn-but-continue","title":"Warn but Continue","text":"<p>Print warning to stderr, then fall back to PassThroughFilter.</p> <p>Pros: User notified of fallback; still allows execution</p> <p>Cons: Warning noise in logs; complicates testing</p> <p>Reason for rejection: Consistent with ADR-0009 reasoning - defer notification mechanism to future enhancement</p>"},{"location":"adr/0011-silent-fallback-on-profile-not-found/#future-direction","title":"Future Direction","text":"<p>Following ADR-0009 future direction:</p> <ul> <li>Add structured logging when profile not found</li> <li>Implement <code>--verbose</code> flag to show profile fallback notifications</li> <li>Implement <code>stym doctor</code> command to validate profile configuration</li> </ul>"},{"location":"adr/0011-silent-fallback-on-profile-not-found/#references","title":"References","text":"<ul> <li>Issue #10: Introduce default profile concept</li> <li>ADR-0009: Silent Fallback to PassThrough Filter on Model Unavailability</li> <li>ADR-0010: Introduce Profile as Core Domain Object</li> </ul>"},{"location":"adr/0012-adopt-repository-pattern-for-profile-access/","title":"ADR 0012: Adopt Repository Pattern for Profile Access","text":""},{"location":"adr/0012-adopt-repository-pattern-for-profile-access/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/0012-adopt-repository-pattern-for-profile-access/#date","title":"Date","text":"<p>2025-12-05</p>"},{"location":"adr/0012-adopt-repository-pattern-for-profile-access/#context","title":"Context","text":"<p>ADR-0010 introduced Profile as a domain object. Profiles need to be retrieved by name and made available to the application.</p> <p>Options for profile access: 1. Direct loading (Profile.load(name)) 2. Service pattern (ProfileService.get(name)) 3. Repository pattern (ProfileRepository.get(name))</p>"},{"location":"adr/0012-adopt-repository-pattern-for-profile-access/#decision","title":"Decision","text":"<p>Use Repository pattern for profile access.</p>"},{"location":"adr/0012-adopt-repository-pattern-for-profile-access/#rationale","title":"Rationale","text":"<ul> <li>Domain-driven design: Repository is the standard pattern for domain object lifecycle management</li> <li>Testability: Easy to inject mock repositories in tests</li> <li>Clear extension point: Future implementations can load from files, databases, or remote APIs</li> <li>Separation of concerns: Repository handles profile retrieval; Application handles business logic</li> </ul>"},{"location":"adr/0012-adopt-repository-pattern-for-profile-access/#implications","title":"Implications","text":""},{"location":"adr/0012-adopt-repository-pattern-for-profile-access/#positive-implications","title":"Positive Implications","text":"<ul> <li>Profile access logic separated from Application lifecycle</li> <li>Easy to test profile resolution independently</li> <li>Repository can be swapped for different storage backends</li> </ul>"},{"location":"adr/0012-adopt-repository-pattern-for-profile-access/#concerns","title":"Concerns","text":"<ul> <li>Additional abstraction (mitigation: Repository is well-understood pattern with clear benefits)</li> </ul>"},{"location":"adr/0012-adopt-repository-pattern-for-profile-access/#alternatives","title":"Alternatives","text":""},{"location":"adr/0012-adopt-repository-pattern-for-profile-access/#direct-loading","title":"Direct Loading","text":"<p>Use static method <code>Profile.load(name)</code>.</p> <p>Pros: Simpler, fewer classes</p> <p>Cons: Tight coupling to storage mechanism; hard to test; violates Single Responsibility Principle</p> <p>Reason for rejection: Repository provides better separation and testability</p>"},{"location":"adr/0012-adopt-repository-pattern-for-profile-access/#service-pattern","title":"Service Pattern","text":"<p>Use <code>ProfileService</code> for profile operations.</p> <p>Pros: Service pattern is flexible</p> <p>Cons: Less specific than Repository; Repository is standard for domain object access</p> <p>Reason for rejection: Repository pattern is more precise for this use case</p>"},{"location":"adr/0012-adopt-repository-pattern-for-profile-access/#references","title":"References","text":"<ul> <li>Issue #10: Introduce default profile concept</li> <li>ADR-0010: Introduce Profile as Core Domain Object</li> <li>Repository Pattern - Martin Fowler</li> </ul>"},{"location":"adr/0013-rename-filter-to-processor/","title":"ADR 0013: Rename Filter to Processor","text":""},{"location":"adr/0013-rename-filter-to-processor/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/0013-rename-filter-to-processor/#date","title":"Date","text":"<p>2025-12-06</p>"},{"location":"adr/0013-rename-filter-to-processor/#context","title":"Context","text":"<p>The term \"Filter\" was introduced in ADR-0007 to represent output transformation logic. While the name was reasonable at the time, it has become apparent that \"Filter\" carries a strong connotation of removal or reduction (like coffee filters, air filters, or UNIX grep).</p> <p>However, shtym's transformation capabilities include: - Reducing: Long output \u2192 summary (fits \"filter\" metaphor) - Expanding: Error messages \u2192 detailed explanations (does not fit \"filter\" metaphor) - Converting: JSON \u2192 human-readable format (does not fit \"filter\" metaphor) - Translating: English \u2192 Japanese (does not fit \"filter\" metaphor)</p> <p>The term \"Filter\" emphasizes only the reduction aspect, which misrepresents the abstraction's full purpose.</p>"},{"location":"adr/0013-rename-filter-to-processor/#decision","title":"Decision","text":"<p>Rename \"Filter\" to \"Processor\" throughout the codebase.</p> <ul> <li><code>Filter</code> Protocol \u2192 <code>Processor</code> Protocol</li> <li><code>PassThroughFilter</code> \u2192 <code>PassThroughProcessor</code></li> <li><code>LLMFilter</code> \u2192 <code>LLMProcessor</code></li> </ul>"},{"location":"adr/0013-rename-filter-to-processor/#rationale","title":"Rationale","text":"<p>\"Processor\" is more accurate: - Neutral term that covers transformation, conversion, expansion, and reduction - Commonly used in software (data processors, text processors, image processors) - Does not imply a specific type of operation</p> <p>Precedent in similar tools: - Text processors (word processors transform text) - Data processors (ETL tools transform data) - Stream processors (transform streaming data)</p> <p>Better alignment with user mental model: - Users configure \"how to process output\", not \"how to filter output\" - Processing includes both simplification and enrichment</p>"},{"location":"adr/0013-rename-filter-to-processor/#implications","title":"Implications","text":""},{"location":"adr/0013-rename-filter-to-processor/#positive-implications","title":"Positive Implications","text":"<ul> <li>More accurate terminology that reflects actual capabilities</li> <li>Easier to explain future features (translation, format conversion)</li> <li>Aligns with common software engineering vocabulary</li> </ul>"},{"location":"adr/0013-rename-filter-to-processor/#concerns","title":"Concerns","text":"<ul> <li>Breaking change for any external documentation or tutorials (mitigation: no public API yet, only internal usage)</li> <li>Existing ADRs use \"Filter\" terminology (mitigation: amendment sections will clarify the rename)</li> </ul>"},{"location":"adr/0013-rename-filter-to-processor/#alternatives","title":"Alternatives","text":""},{"location":"adr/0013-rename-filter-to-processor/#keep-filter-and-expand-its-definition","title":"Keep \"Filter\" and expand its definition","text":"<p>Pros: No renaming needed, term already established in ADRs</p> <p>Cons: Fighting against common understanding of \"filter\"; confusing to new contributors</p> <p>Reason for rejection: Misleading terminology is worse than a one-time rename</p>"},{"location":"adr/0013-rename-filter-to-processor/#use-transformer","title":"Use \"Transformer\"","text":"<p>Pros: Clear transformation intent</p> <p>Cons: Overloaded term (ML transformers, electrical transformers, design pattern)</p> <p>Reason for rejection: \"Processor\" is less ambiguous</p>"},{"location":"adr/0013-rename-filter-to-processor/#references","title":"References","text":"<ul> <li>ADR-0007: Introduce Filter Abstraction for Output Processing (uses \"Filter\" terminology)</li> <li>ADR-0009: Silent Fallback to PassThrough Filter (uses \"Filter\" terminology)</li> </ul>"},{"location":"adr/0014-introduce-processor-factory-functions-in-domain-layer/","title":"ADR 0014: Introduce Processor Factory Functions in Domain Layer","text":""},{"location":"adr/0014-introduce-processor-factory-functions-in-domain-layer/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/0014-introduce-processor-factory-functions-in-domain-layer/#date","title":"Date","text":"<p>2025-12-08</p>"},{"location":"adr/0014-introduce-processor-factory-functions-in-domain-layer/#context","title":"Context","text":"<p>Application layer needs to create Processors from Profiles. The creation logic involves: - Retrieving Profile from repository - Handling ProfileNotFoundError - Creating Processor from Profile via ProcessorFactory - Wrapping Processor with fallback logic - Handling ProcessorCreationError</p> <p>Without domain-level factory functions, this complex creation logic would be duplicated across multiple places (CLI, tests, future API layer).</p>"},{"location":"adr/0014-introduce-processor-factory-functions-in-domain-layer/#decision","title":"Decision","text":"<p>Introduce two factory functions in domain layer:</p> <pre><code># domain/processor.py\ndef create_processor_with_fallback(\n    profile: Profile, processor_factory: ProcessorFactory\n) -&gt; Processor:\n    \"\"\"Create processor with automatic fallback to PassThroughProcessor.\"\"\"\n\ndef create_processor_from_profile_name(\n    profile_name: str,\n    profile_repository: ProfileRepository,\n    processor_factory: ProcessorFactory,\n) -&gt; Processor:\n    \"\"\"Create processor from profile name with automatic fallback.\"\"\"\n</code></pre>"},{"location":"adr/0014-introduce-processor-factory-functions-in-domain-layer/#rationale","title":"Rationale","text":"<ul> <li>Single Responsibility: Each function handles one creation scenario</li> <li>Reusability: Application, CLI, and tests can use same creation logic</li> <li>Domain logic in domain layer: Fallback decision is business logic, not application orchestration</li> <li>Dependency injection: Functions accept repository and factory as parameters, enabling testability</li> </ul>"},{"location":"adr/0014-introduce-processor-factory-functions-in-domain-layer/#implications","title":"Implications","text":""},{"location":"adr/0014-introduce-processor-factory-functions-in-domain-layer/#positive-implications","title":"Positive Implications","text":"<ul> <li>Application layer simplified to single function call</li> <li>Consistent fallback behavior across all entry points</li> <li>Easy to test creation logic in isolation</li> <li>Clear extension point for future Processor creation variations</li> </ul>"},{"location":"adr/0014-introduce-processor-factory-functions-in-domain-layer/#concerns","title":"Concerns","text":"<ul> <li>Domain layer now depends on ProfileRepository abstraction (mitigation: repository is domain abstraction, this is acceptable)</li> <li>Factory functions increase domain layer API surface (mitigation: only two focused functions, both essential)</li> </ul>"},{"location":"adr/0014-introduce-processor-factory-functions-in-domain-layer/#alternatives","title":"Alternatives","text":""},{"location":"adr/0014-introduce-processor-factory-functions-in-domain-layer/#application-layer-handles-creation","title":"Application Layer Handles Creation","text":"<p>Place creation logic in Application.create() method.</p> <p>Pros: Simpler domain layer, orchestration in application layer</p> <p>Cons: Cannot reuse creation logic in CLI or other contexts; tests must test through Application</p> <p>Reason for rejection: Creation logic is business rule (when to fallback), belongs in domain</p>"},{"location":"adr/0014-introduce-processor-factory-functions-in-domain-layer/#builder-pattern","title":"Builder Pattern","text":"<p>Use builder pattern for Processor creation.</p> <p>Pros: Fluent API, can chain configuration</p> <p>Cons: Overkill for two simple creation scenarios; more complex than needed</p> <p>Reason for rejection: Factory functions provide sufficient flexibility with less ceremony</p>"},{"location":"adr/0014-introduce-processor-factory-functions-in-domain-layer/#references","title":"References","text":"<ul> <li>Issue #10: Introduce default profile concept</li> <li>ADR-0011: Silent Fallback on Profile Not Found</li> <li>ADR-0012: Adopt Repository Pattern for Profile Access</li> </ul>"},{"location":"adr/0015-adopt-factory-pattern-for-profile-to-processor-conversion/","title":"ADR 0015: Adopt Factory Pattern for Profile-to-Processor Conversion","text":""},{"location":"adr/0015-adopt-factory-pattern-for-profile-to-processor-conversion/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/0015-adopt-factory-pattern-for-profile-to-processor-conversion/#date","title":"Date","text":"<p>2025-12-08</p>"},{"location":"adr/0015-adopt-factory-pattern-for-profile-to-processor-conversion/#context","title":"Context","text":"<p>Profiles (domain objects) contain configuration for output transformation. Processors (infrastructure implementations) perform the actual transformation.</p> <p>The application needs to convert Profile \u2192 Processor. This conversion involves: - Type-based dispatch (LLMProfile \u2192 LLMProcessor, future profiles \u2192 other processors) - Infrastructure-specific instantiation (LLM clients, API connections, etc.) - Error handling for unsupported profile types</p> <p>Without abstraction, domain layer would need to know about all infrastructure implementations.</p>"},{"location":"adr/0015-adopt-factory-pattern-for-profile-to-processor-conversion/#decision","title":"Decision","text":"<p>Introduce ProcessorFactory protocol in domain layer and ConcreteProcessorFactory in infrastructure layer.</p> <pre><code># domain/processor.py\nclass ProcessorFactory(Protocol):\n    def create(self, profile: Profile) -&gt; Processor: ...\n\n# infrastructure/processors/factory.py\nclass ConcreteProcessorFactory:\n    def create(self, profile: Profile) -&gt; Processor:\n        if isinstance(profile, LLMProfile):\n            return LLMProcessor.create(profile=profile)\n        raise ProcessorCreationError(...)\n</code></pre>"},{"location":"adr/0015-adopt-factory-pattern-for-profile-to-processor-conversion/#rationale","title":"Rationale","text":"<ul> <li>Dependency Inversion: Domain depends on ProcessorFactory abstraction, not concrete implementations</li> <li>Open/Closed: Adding new profile types requires only new infrastructure code, no domain changes</li> <li>Type-based dispatch: Factory centralizes profile-type-to-processor-implementation mapping</li> <li>Encapsulation: Infrastructure creation details hidden from domain layer</li> </ul>"},{"location":"adr/0015-adopt-factory-pattern-for-profile-to-processor-conversion/#implications","title":"Implications","text":""},{"location":"adr/0015-adopt-factory-pattern-for-profile-to-processor-conversion/#positive-implications","title":"Positive Implications","text":"<ul> <li>Domain layer remains ignorant of infrastructure implementations</li> <li>Easy to add new profile types (OpenAI, Claude, custom processors)</li> <li>Factory is single point for processor instantiation logic</li> <li>Testable via mock factories in domain tests</li> </ul>"},{"location":"adr/0015-adopt-factory-pattern-for-profile-to-processor-conversion/#concerns","title":"Concerns","text":"<ul> <li>Adds abstraction layer between Profile and Processor (mitigation: necessary for dependency inversion)</li> <li>Factory must know all profile types (mitigation: limited to infrastructure layer, acceptable coupling)</li> </ul>"},{"location":"adr/0015-adopt-factory-pattern-for-profile-to-processor-conversion/#alternatives","title":"Alternatives","text":""},{"location":"adr/0015-adopt-factory-pattern-for-profile-to-processor-conversion/#direct-profile-to-processor-method","title":"Direct Profile-to-Processor Method","text":"<p>Add <code>to_processor()</code> method on Profile base class.</p> <pre><code>class Profile:\n    def to_processor(self) -&gt; Processor: ...\n</code></pre> <p>Pros: No separate factory needed, simple API</p> <p>Cons: Domain objects create infrastructure objects, violates layering; Profile must import infrastructure</p> <p>Reason for rejection: Violates dependency inversion and layered architecture</p>"},{"location":"adr/0015-adopt-factory-pattern-for-profile-to-processor-conversion/#service-locator-pattern","title":"Service Locator Pattern","text":"<p>Use service locator to find processor for given profile.</p> <p>Pros: Decouples profile from processor creation</p> <p>Cons: Hidden dependencies, harder to test, anti-pattern in modern design</p> <p>Reason for rejection: Factory pattern is more explicit and testable</p>"},{"location":"adr/0015-adopt-factory-pattern-for-profile-to-processor-conversion/#references","title":"References","text":"<ul> <li>Issue #10: Introduce default profile concept</li> <li>ADR-0010: Introduce Profile as Core Domain Object</li> <li>Factory Pattern</li> </ul>"},{"location":"adr/0016-use-dynamic-module-loading-for-llm-client-creation/","title":"ADR 0016: Use Dynamic Module Loading for LLM Client Creation","text":""},{"location":"adr/0016-use-dynamic-module-loading-for-llm-client-creation/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/0016-use-dynamic-module-loading-for-llm-client-creation/#date","title":"Date","text":"<p>2025-12-08</p>"},{"location":"adr/0016-use-dynamic-module-loading-for-llm-client-creation/#context","title":"Context","text":"<p>LLMProcessor requires an LLMClient (e.g., OllamaLLMClient) to function. The ollama library is an optional dependency - users may not have it installed if they don't use LLM features.</p> <p>Without guards, importing infrastructure code would fail with ImportError when ollama is not installed, breaking the entire application even for users who only want PassThroughProcessor.</p>"},{"location":"adr/0016-use-dynamic-module-loading-for-llm-client-creation/#decision","title":"Decision","text":"<p>Use importlib for dynamic module loading in LLMClientFactory.</p> <pre><code># infrastructure/llm_clients/factory.py\nclass LLMClientFactory:\n    def create(self, profile: BaseLLMClientSettings) -&gt; LLMClient:\n        if isinstance(profile, OllamaLLMClientSettings):\n            try:\n                ollama_client_module = importlib.import_module(\n                    \"shtym.infrastructure.llm_clients.ollama_client\"\n                )\n                return ollama_client_module.OllamaLLMClient.create(settings=profile)\n            except ImportError as e:\n                raise LLMModuleNotFoundError(\"ollama\") from e\n</code></pre>"},{"location":"adr/0016-use-dynamic-module-loading-for-llm-client-creation/#rationale","title":"Rationale","text":"<ul> <li>Graceful degradation: Application works without ollama library installed</li> <li>Delayed import: Only imports LLM modules when actually needed</li> <li>Clear error messages: LLMModuleNotFoundError provides actionable feedback</li> <li>Zero-configuration default: PassThroughProcessor users don't need LLM dependencies</li> </ul>"},{"location":"adr/0016-use-dynamic-module-loading-for-llm-client-creation/#implications","title":"Implications","text":""},{"location":"adr/0016-use-dynamic-module-loading-for-llm-client-creation/#positive-implications","title":"Positive Implications","text":"<ul> <li>Application installable and usable without LLM dependencies</li> <li>LLM features opt-in via optional dependency group</li> <li>Clear error when LLM requested but dependencies missing</li> <li>Follows principle of \"fail late\" for optional features</li> </ul>"},{"location":"adr/0016-use-dynamic-module-loading-for-llm-client-creation/#concerns","title":"Concerns","text":"<ul> <li>Dynamic imports harder to trace statically (mitigation: only used in factory, well-documented)</li> <li>Type checkers may not catch import errors (mitigation: tests cover import failure scenarios)</li> <li>Slightly more complex than static imports (mitigation: complexity isolated in factory)</li> </ul>"},{"location":"adr/0016-use-dynamic-module-loading-for-llm-client-creation/#alternatives","title":"Alternatives","text":""},{"location":"adr/0016-use-dynamic-module-loading-for-llm-client-creation/#static-imports-with-tryexcept-at-module-level","title":"Static Imports with Try/Except at Module Level","text":"<p>Import ollama at module top level, wrap in try/except.</p> <pre><code>try:\n    from ollama import Client\n    OLLAMA_AVAILABLE = True\nexcept ImportError:\n    OLLAMA_AVAILABLE = False\n</code></pre> <p>Pros: Simpler, checked once at import time</p> <p>Cons: Fails entire module if import fails; cannot provide user-facing error at usage time</p> <p>Reason for rejection: Want to provide clear error message when user tries to use LLM, not when module loads</p>"},{"location":"adr/0016-use-dynamic-module-loading-for-llm-client-creation/#optional-dependency-with-runtime-check","title":"Optional Dependency with Runtime Check","text":"<p>Make ollama required dependency, check availability at runtime.</p> <p>Pros: Simplest implementation, static imports</p> <p>Cons: Forces all users to install ollama even if not using LLM features</p> <p>Reason for rejection: Violates zero-configuration principle for PassThrough users</p>"},{"location":"adr/0016-use-dynamic-module-loading-for-llm-client-creation/#plugin-architecture","title":"Plugin Architecture","text":"<p>Use plugin system to load LLM clients dynamically.</p> <p>Pros: Maximum flexibility, extensible</p> <p>Cons: Over-engineered for current needs; adds significant complexity</p> <p>Reason for rejection: importlib provides sufficient flexibility with minimal overhead</p>"},{"location":"adr/0016-use-dynamic-module-loading-for-llm-client-creation/#references","title":"References","text":"<ul> <li>Issue #10: Introduce default profile concept</li> <li>ADR-0009: Silent Fallback to PassThrough Processor on Model Unavailability</li> <li>Python importlib documentation</li> </ul>"},{"location":"adr/0017-use-custom-exception-hierarchy-for-infrastructure-errors/","title":"ADR 0017: Use Custom Exception Hierarchy for Infrastructure Errors","text":""},{"location":"adr/0017-use-custom-exception-hierarchy-for-infrastructure-errors/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/0017-use-custom-exception-hierarchy-for-infrastructure-errors/#date","title":"Date","text":"<p>2025-12-16</p>"},{"location":"adr/0017-use-custom-exception-hierarchy-for-infrastructure-errors/#context","title":"Context","text":"<p>Infrastructure layer components interact with external systems such as file systems, networks, and configuration parsers. These operations can fail for various reasons (file not found, permission denied, network timeout, parse errors).</p> <p>Python provides built-in exceptions like <code>FileNotFoundError</code>, <code>OSError</code>, and <code>ValueError</code> that can be raised directly. However, using Python built-in exceptions directly in the infrastructure layer creates inconsistency when some infrastructure components use domain-specific exceptions while others use built-ins.</p> <p>This inconsistency makes error handling less uniform and harder to reason about. Callers need to catch both built-in exceptions and domain-specific exceptions, and it's unclear which layer is responsible for which error type.</p>"},{"location":"adr/0017-use-custom-exception-hierarchy-for-infrastructure-errors/#decision","title":"Decision","text":"<p>All infrastructure layer errors must extend <code>ShtymInfrastructureError</code>, which itself extends the base <code>ShtymError</code> exception class.</p> <p>Infrastructure components must catch Python built-in exceptions (such as <code>FileNotFoundError</code>, <code>OSError</code>, <code>ValueError</code>) at the boundary and re-raise them as domain-specific exceptions that extend <code>ShtymInfrastructureError</code>.</p>"},{"location":"adr/0017-use-custom-exception-hierarchy-for-infrastructure-errors/#rationale","title":"Rationale","text":"<p>Consistency: All infrastructure errors follow the same pattern, making error handling uniform across the codebase.</p> <p>Clear layer boundaries: The exception hierarchy explicitly marks which errors come from which architectural layer. Infrastructure errors are distinguishable from domain errors and application errors.</p> <p>Simplified error handling: Callers can catch <code>ShtymInfrastructureError</code> to handle all infrastructure failures without needing to know about specific Python built-in exceptions.</p> <p>Extensibility: New infrastructure error types can be added by extending <code>ShtymInfrastructureError</code>, maintaining consistency as the system evolves.</p>"},{"location":"adr/0017-use-custom-exception-hierarchy-for-infrastructure-errors/#implications","title":"Implications","text":""},{"location":"adr/0017-use-custom-exception-hierarchy-for-infrastructure-errors/#positive-implications","title":"Positive Implications","text":"<ul> <li>Exception hierarchy clearly reflects architectural layers</li> <li>Error handling code is more consistent and predictable</li> <li>Infrastructure errors can be handled uniformly by upper layers</li> <li>New contributors can easily understand which exceptions belong to which layer</li> </ul>"},{"location":"adr/0017-use-custom-exception-hierarchy-for-infrastructure-errors/#concerns","title":"Concerns","text":"<ul> <li>Adds boilerplate code to catch and re-raise built-in exceptions (mitigation: consistent pattern makes copy-paste straightforward; future helper utilities can reduce boilerplate if needed)</li> <li>Each new infrastructure component needs to define its own exception class (mitigation: clear naming convention and template pattern; existing examples serve as reference)</li> <li>Exception chaining must be used (<code>raise ... from e</code>) to preserve original error information (mitigation: linters and code review enforce proper chaining; tests verify stack traces)</li> </ul>"},{"location":"adr/0017-use-custom-exception-hierarchy-for-infrastructure-errors/#alternatives","title":"Alternatives","text":""},{"location":"adr/0017-use-custom-exception-hierarchy-for-infrastructure-errors/#use-python-built-in-exceptions-directly","title":"Use Python Built-in Exceptions Directly","text":"<p>Allow infrastructure components to raise Python built-in exceptions like <code>FileNotFoundError</code>, <code>OSError</code>, <code>ValueError</code> without wrapping them.</p> <p>Pros: Simpler implementation with no wrapper exceptions needed; less boilerplate code</p> <p>Cons: Creates inconsistency when some components use domain exceptions; makes layer boundaries unclear; callers must know about both built-in and custom exceptions</p> <p>Reason for rejection: Inconsistent with existing domain-specific exceptions like ProfileParserError; violates layered architecture principle</p>"},{"location":"adr/0017-use-custom-exception-hierarchy-for-infrastructure-errors/#mix-built-in-and-custom-exceptions-based-on-error-type","title":"Mix Built-in and Custom Exceptions Based on Error Type","text":"<p>Allow some exceptions to be built-ins (e.g., file errors) while others are custom (e.g., parsing errors).</p> <p>Pros: Flexible approach; uses built-ins where they fit naturally</p> <p>Cons: Creates confusion about which errors use which pattern; inconsistent error handling across infrastructure layer; unclear guidelines for contributors</p> <p>Reason for rejection: Consistency is more valuable than flexibility; uniform pattern easier to understand and maintain</p>"},{"location":"adr/0017-use-custom-exception-hierarchy-for-infrastructure-errors/#create-exception-wrappers-only-at-application-boundaries","title":"Create Exception Wrappers Only at Application Boundaries","text":"<p>Let infrastructure layer raise built-in exceptions; convert to custom exceptions only at application entry points.</p> <p>Pros: Minimal boilerplate in infrastructure layer; conversion logic centralized</p> <p>Cons: Pushes error handling responsibility to application layer; doesn't clearly separate infrastructure concerns; leaks implementation details across layers</p> <p>Reason for rejection: Violates separation of concerns; each layer should own its error types</p>"},{"location":"adr/0017-use-custom-exception-hierarchy-for-infrastructure-errors/#future-direction","title":"Future Direction","text":"<p>As new infrastructure components are added (network clients, database adapters, external API integrations), each must define its own exception class extending <code>ShtymInfrastructureError</code>.</p> <p>If exception handling becomes overly verbose, consider introducing helper utilities to reduce boilerplate while maintaining the custom exception hierarchy.</p>"},{"location":"adr/0017-use-custom-exception-hierarchy-for-infrastructure-errors/#references","title":"References","text":"<ul> <li>ADR-0003: Adopt Layered Architecture - establishes the infrastructure layer</li> <li>Python Exception Hierarchy: https://docs.python.org/3/library/exceptions.html</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>This document provides a high-level overview of architectural decisions made for the shtym project.</p>"},{"location":"architecture/overview/#architecture-decision-records","title":"Architecture Decision Records","text":""},{"location":"architecture/overview/#adr-0001-keep-stdout-clean-for-ai-and-human-consumption","title":"ADR-0001: Keep stdout Clean for AI and Human Consumption","text":"<p>Status: Accepted | Date: 2025-11-20</p> <p>Keep stdout exclusively for AI-generated summaries, directing all other output to stderr for clean consumption by humans and AI agents.</p>"},{"location":"architecture/overview/#adr-0002-use-argparse-for-cli-implementation","title":"ADR-0002: Use argparse for CLI Implementation","text":"<p>Status: Accepted | Date: 2025-11-20</p> <p>Use Python's standard library argparse for CLI parsing to maintain zero external dependencies and align with the clean output philosophy.</p>"},{"location":"architecture/overview/#adr-0003-adopt-layered-architecture","title":"ADR-0003: Adopt Layered Architecture","text":"<p>Status: Accepted | Date: 2025-11-21</p> <p>Adopt a four-layer architecture (Presentation, Application, Domain, Infrastructure) to separate concerns, enable testing, and prepare for future LLM integration.</p>"},{"location":"architecture/overview/#adr-0004-do-not-implement-stdin-pipe-mode","title":"ADR-0004: Do Not Implement Stdin Pipe Mode","text":"<p>Status: Accepted | Date: 2025-11-21</p> <p>Use wrapper mode (<code>stym run command</code>) instead of pipe mode (<code>command | stym</code>) to enable exit code inheritance, which is critical for CI/CD integration.</p>"},{"location":"architecture/overview/#adr-0005-inherit-child-process-exit-codes","title":"ADR-0005: Inherit Child Process Exit Codes","text":"<p>Status: Accepted | Date: 2025-11-21</p> <p>Shtym must inherit and propagate the child process exit code exactly as-is, following Unix wrapper command conventions (sudo, timeout, time) for seamless CI/CD integration.</p>"},{"location":"architecture/overview/#adr-0006-adopt-subcommand-architecture","title":"ADR-0006: Adopt Subcommand Architecture","text":"<p>Status: Accepted | Date: 2025-11-21</p> <p>Adopt subcommand architecture (<code>stym run</code>, <code>stym status</code>, <code>stym config</code>) to enable future features without breaking changes or command ambiguity.</p>"},{"location":"architecture/overview/#adr-0007-introduce-filter-abstraction-for-output-processing","title":"ADR-0007: Introduce Filter Abstraction for Output Processing","text":"<p>Status: Accepted | Date: 2025-11-21</p> <p>Introduce Filter protocol with PassThroughFilter implementation now to minimize code changes when adding LLM integration, following Dependency Injection and SOLID principles.</p>"},{"location":"architecture/overview/#adr-0008-introduce-llm-client-abstraction","title":"ADR-0008: Introduce LLM Client Abstraction","text":"<p>Status: Accepted (Amended 2025-12-08) | Date: 2025-12-02</p> <p>Introduce LLMClient protocol as internal infrastructure abstraction and OllamaLLMClient implementation to decouple LLM processor from specific providers. Amendment clarifies LLMClient belongs in infrastructure layer, not domain layer.</p>"},{"location":"architecture/overview/#adr-0009-silent-fallback-to-passthrough-filter-on-model-unavailability","title":"ADR-0009: Silent Fallback to PassThrough Filter on Model Unavailability","text":"<p>Status: Accepted | Date: 2025-12-04</p> <p>When configured LLM model is unavailable, silently fall back to PassThroughFilter without warnings or errors, prioritizing graceful degradation and zero-configuration experience over strict validation.</p>"},{"location":"architecture/overview/#adr-0010-introduce-profile-as-core-domain-object","title":"ADR-0010: Introduce Profile as Core Domain Object","text":"<p>Status: Accepted | Date: 2025-12-05</p> <p>Introduce \"Profile\" as a core domain concept representing a named configuration for output transformation, providing a stable user-facing abstraction independent of implementation changes.</p>"},{"location":"architecture/overview/#adr-0011-silent-fallback-on-profile-not-found","title":"ADR-0011: Silent Fallback on Profile Not Found","text":"<p>Status: Accepted | Date: 2025-12-05</p> <p>When requested profile does not exist, silently fall back to PassThroughFilter without warnings or errors, consistent with ADR-0009's graceful degradation pattern.</p>"},{"location":"architecture/overview/#adr-0012-adopt-repository-pattern-for-profile-access","title":"ADR-0012: Adopt Repository Pattern for Profile Access","text":"<p>Status: Accepted | Date: 2025-12-05</p> <p>Use Repository pattern for profile access to separate profile retrieval logic from application business logic, enabling testability and future storage backend flexibility.</p>"},{"location":"architecture/overview/#adr-0013-rename-filter-to-processor","title":"ADR-0013: Rename Filter to Processor","text":"<p>Status: Accepted | Date: 2025-12-06</p> <p>Rename \"Filter\" to \"Processor\" to better reflect the abstraction's purpose of transforming output (expansion, conversion, translation) rather than only filtering/reducing it.</p>"},{"location":"architecture/overview/#adr-0014-introduce-processor-factory-functions-in-domain-layer","title":"ADR-0014: Introduce Processor Factory Functions in Domain Layer","text":"<p>Status: Accepted | Date: 2025-12-08</p> <p>Introduce domain-level factory functions (create_processor_with_fallback, create_processor_from_profile_name) to centralize processor creation logic and ensure consistent fallback behavior across all entry points.</p>"},{"location":"architecture/overview/#adr-0015-adopt-factory-pattern-for-profile-to-processor-conversion","title":"ADR-0015: Adopt Factory Pattern for Profile-to-Processor Conversion","text":"<p>Status: Accepted | Date: 2025-12-08</p> <p>Introduce ProcessorFactory protocol in domain layer and ConcreteProcessorFactory in infrastructure layer to decouple profile configuration from processor instantiation, following dependency inversion principle.</p>"},{"location":"architecture/overview/#adr-0016-use-dynamic-module-loading-for-llm-client-creation","title":"ADR-0016: Use Dynamic Module Loading for LLM Client Creation","text":"<p>Status: Accepted | Date: 2025-12-08</p> <p>Use importlib for dynamic module loading in LLMClientFactory to enable graceful degradation when optional LLM dependencies are not installed, supporting zero-configuration usage of PassThroughProcessor.</p>"},{"location":"architecture/overview/#adr-0017-use-custom-exception-hierarchy-for-infrastructure-errors","title":"ADR-0017: Use Custom Exception Hierarchy for Infrastructure Errors","text":"<p>Status: Accepted | Date: 2025-12-16</p> <p>All infrastructure layer errors must extend ShtymInfrastructureError to ensure consistent error handling, clear layer boundaries, and uniform exception patterns across the codebase.</p>"}]}